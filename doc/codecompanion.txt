*codecompanion.txt*          For NVIM v0.11          Last change: 2025 July 11

==============================================================================
Table of Contents                            *codecompanion-table-of-contents*

1. Welcome                                             |codecompanion-welcome|
  - Features                                  |codecompanion-welcome-features|
  - Overview                                  |codecompanion-welcome-overview|
2. Installation                                   |codecompanion-installation|
  - Requirements                     |codecompanion-installation-requirements|
  - Installation                     |codecompanion-installation-installation|
  - Installing Extensions   |codecompanion-installation-installing-extensions|
  - Additional s                     |codecompanion-installation-additional-s|
  - Completion                         |codecompanion-installation-completion|
  - Help                                     |codecompanion-installation-help|
3. Getting Started                             |codecompanion-getting-started|
  - With an Adapter            |codecompanion-getting-started-with-an-adapter|
  - Chat Buffer                    |codecompanion-getting-started-chat-buffer|
  - Inline Assistant          |codecompanion-getting-started-inline-assistant|
  - Commands                          |codecompanion-getting-started-commands|
  - Action Palette              |codecompanion-getting-started-action-palette|
  - List of Commands          |codecompanion-getting-started-list-of-commands|
  - Suggested Workflow      |codecompanion-getting-started-suggested-workflow|
4. Configuration                                 |codecompanion-configuration|
  - General                              |codecompanion-configuration-general|
  - Action Palette                |codecompanion-configuration-action-palette|
  - Adapters                            |codecompanion-configuration-adapters|
  - Chat Buffer                      |codecompanion-configuration-chat-buffer|
  - Inline Assistant            |codecompanion-configuration-inline-assistant|
  - Prompt Library                |codecompanion-configuration-prompt-library|
  - System Prompt                  |codecompanion-configuration-system-prompt|
  - Extensions                        |codecompanion-configuration-extensions|
  - Other Options                  |codecompanion-configuration-other-options|
5. Usage                                                 |codecompanion-usage|
  - General                                      |codecompanion-usage-general|
  - Action Palette                        |codecompanion-usage-action-palette|
  - Chat Buffer                              |codecompanion-usage-chat-buffer|
  - Agents and Tools                    |codecompanion-usage-agents-and-tools|
  - Slash Commands                        |codecompanion-usage-slash-commands|
  - Variables                                  |codecompanion-usage-variables|
  - Events / Hooks                        |codecompanion-usage-events-/-hooks|
  - Inline Assistant                    |codecompanion-usage-inline-assistant|
  - User Interface                        |codecompanion-usage-user-interface|
  - Workflows                                  |codecompanion-usage-workflows|
6. Extending                                         |codecompanion-extending|
  - Creating Adapters              |codecompanion-extending-creating-adapters|
  - Creating Prompts                |codecompanion-extending-creating-prompts|
  - Creating Tools                    |codecompanion-extending-creating-tools|
  - Creating Workflows            |codecompanion-extending-creating-workflows|
  - Creating Workspaces          |codecompanion-extending-creating-workspaces|
  - Creating Extensions          |codecompanion-extending-creating-extensions|

==============================================================================
1. Welcome                                             *codecompanion-welcome*


  AI-powered coding, seamlessly in `Neovim`
CodeCompanion is a productivity tool which streamlines how you develop with
LLMs, in Neovim.


FEATURES                                      *codecompanion-welcome-features*

-  Copilot Chat <https://github.com/features/copilot> meets Zed AI <https://zed.dev/blog/zed-ai>, in Neovim
-  Support for Anthropic, Copilot, GitHub Models, DeepSeek, Gemini, Mistral AI, Novita, Ollama, OpenAI, Azure OpenAI, HuggingFace and xAI LLMs out of the box (or bring your own!)
-  User contributed and supported |codecompanion-configuration-adapters-community-adapters|
-  |codecompanion-usage-inline-assistant.html|, code creation and refactoring
-  |codecompanion-usage-chat-buffer-variables|, |codecompanion-usage-chat-buffer-slash-commands|, |codecompanion-usage-chat-buffer-agents| and |codecompanion-usage-workflows| to improve LLM output
-  Built in |codecompanion-usage-action-palette.html| for common tasks like advice on LSP errors and code explanations
-  Create your own |codecompanion-extending-prompts|, Variables and Slash Commands
-  Have |codecompanion-usage-introduction-quickly-accessing-a-chat-buffer| open at the same time
-  Support for |codecompanion-usage-chat-buffer--images-vision| as input
-  Async execution for fast performance


OVERVIEW                                      *codecompanion-welcome-overview*

The plugin uses |codecompanion-configuration-adapters| to connect to LLMs. Out
of the box, the plugin supports:

- Anthropic (`anthropic`) - Requires an API key and supports prompt caching <https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching>
- Copilot (`copilot`) - Requires a token which is created via `:Copilot setup` in Copilot.vim <https://github.com/github/copilot.vim>
- GitHub Models (`githubmodels`) - Requires `gh` <https://github.com/cli/cli> to be installed and logged in
- DeepSeek (`deepseek`) - Requires an API key
- Gemini (`gemini`) - Requires an API key
- HuggingFace (`huggingface`) - Requires an API key
- Mistral AI (`mistral`) - Requires an API key
- Novita (`novita`) - Requires an API key
- Ollama (`ollama`) - Both local and remotely hosted
- OpenAI (`openai`) - Requires an API key
- Azure OpenAI (`azure_openai`) - Requires an Azure OpenAI service with a model deployment
- xAI (`xai`) - Requires an API key

The plugin utilises objects called `strategies`. These are the different ways
that a user can interact with the plugin. The `chat` strategy harnesses a
buffer to allow direct conversation with the LLM. The `inline` strategy allows
for output from the LLM to be written directly into a pre-existing Neovim
buffer.

The plugin allows you to specify adapters for each strategy and also for each
|codecompanion-configuration-prompt-library| entry.


==============================================================================
2. Installation                                   *codecompanion-installation*


  [!IMPORTANT] The plugin requires the markdown and markdown_inline Tree-sitter
  parsers to be installed with `:TSInstall markdown markdown_inline`

REQUIREMENTS                         *codecompanion-installation-requirements*

- The `curl` library
- Neovim 0.11.0 or greater
- `(Optional)` An API key for your chosen LLM
- `(Optional)` The `base64` library for image/vision support
- `(Optional)` The ripgrep <https://github.com/BurntSushi/ripgrep> library for the `grep_search` tool


INSTALLATION                         *codecompanion-installation-installation*

The plugin can be installed with the plugin manager of your choice:


LAZY.NVIM ~

>lua
    {
      "olimorris/codecompanion.nvim",
      opts = {},
      dependencies = {
        "nvim-lua/plenary.nvim",
        "nvim-treesitter/nvim-treesitter",
      },
    },
<


PACKER ~

>lua
    use({
      "olimorris/codecompanion.nvim",
      config = function()
        require("codecompanion").setup()
      end,
      requires = {
        "nvim-lua/plenary.nvim",
        "nvim-treesitter/nvim-treesitter",
      }
    }),
<


VIM-PLUG ~

>vim
    call plug#begin()
    
    Plug 'nvim-lua/plenary.nvim'
    Plug 'nvim-treesitter/nvim-treesitter'
    Plug 'olimorris/codecompanion.nvim'
    
    call plug#end()
    
    lua << EOF
      require("codecompanion").setup()
    EOF
<

**Pinned plugins**

As per #377 <https://github.com/olimorris/codecompanion.nvim/issues/377>, if
you pin your plugins to the latest releases, ensure you set plenary.nvim to
follow the master branch:

>lua
    { "nvim-lua/plenary.nvim", branch = "master" },
<


INSTALLING EXTENSIONS       *codecompanion-installation-installing-extensions*

CodeCompanion supports extensions that add additional functionality to the
plugin. Below is an example which installs and configures mcphub.nvim
<https://github.com/ravitemer/mcphub.nvim>:

1. Install with:

>lua
    {
      "olimorris/codecompanion.nvim",
      dependencies = {
        "ravitemer/mcphub.nvim"
      }
    }
<

1. Configure with additional options:

>lua
    require("codecompanion").setup({
      extensions = {
        mcphub = {
          callback = "mcphub.extensions.codecompanion",
          opts = {
            make_vars = true,
            make_slash_commands = true,
            show_result_in_chat = true
          }
        }
      }
    })
<

Visit the |codecompanion-extending-extensions| to learn more about available
extensions and how to create your own.


ADDITIONAL S                         *codecompanion-installation-additional-s*

CodeCompanion integrates with a number of other plugins to make your AI coding
experience more enjoyable. Below are some common lazy.nvim configurations for
popular plugins:


RENDER-MARKDOWN.NVIM ~

Use render-markdown.nvim
<https://github.com/MeanderingProgrammer/render-markdown.nvim> to render the
markdown in the chat buffer:

>lua
    {
      "MeanderingProgrammer/render-markdown.nvim",
      ft = { "markdown", "codecompanion" }
    },
<


MARKVIEW.NVIM ~

Use markview.nvim <https://github.com/OXY2DEV/markview.nvim> to render the
markdown in the chat buffer:

>lua
    {
      "OXY2DEV/markview.nvim",
      lazy = false,
      opts = {
        preview = {
          filetypes = { "markdown", "codecompanion" },
          ignore_buftypes = {},
        },
      },
    },
<

You can also conceal the tags used in chat buffers, see HTML options
<https://github.com/OXY2DEV/markview.nvim/wiki/HTML#container_elements> and
this discussion
<https://github.com/olimorris/codecompanion.nvim/discussions/1638> for
examples.


MINI.DIFF ~

Use mini.diff <https://github.com/echasnovski/mini.diff> for a cleaner diff
when using the inline assistant or the `@insert_edit_into_file` tool:

>lua
    {
      "echasnovski/mini.diff",
      config = function()
        local diff = require("mini.diff")
        diff.setup({
          -- Disabled by default
          source = diff.gen_source.none(),
        })
      end,
    },
<


IMG-CLIP.NVIM ~

Use img-clip.nvim <https://github.com/hakonharnes/img-clip.nvim> to copy images
from your system clipboard into a chat buffer via `:PasteImage`:

>lua
    {
      "HakonHarnes/img-clip.nvim",
      opts = {
        filetypes = {
          codecompanion = {
            prompt_for_file_name = false,
            template = "[Image]($FILE_PATH)",
            use_absolute_path = true,
          },
        },
      },
    },
<


COMPLETION                             *codecompanion-installation-completion*

When in the |codecompanion-usage-chat-buffer-index|, completion can be used to
more easily add |codecompanion-usage-chat-buffer-variables|,
|codecompanion-usage-chat-buffer-slash-commands| and
|codecompanion-usage-chat-buffer-agents|. Out of the box, the plugin supports
completion with both nvim-cmp <https://github.com/hrsh7th/nvim-cmp> and
blink.cmp <https://github.com/Saghen/blink.cmp>. For the latter, on version <=
0.10.0, ensure that you‚Äôve added `codecompanion` as a source:

>lua
    sources = {
      per_filetype = {
        codecompanion = { "codecompanion" },
      }
    },
<

The plugin also supports |codecompanion-usage-chat-buffer-index-completion| and
coc.nvim <https://github.com/neoclide/coc.nvim>.


HELP                                         *codecompanion-installation-help*

If you‚Äôre having trouble installing the plugin, as a first step, run
`:checkhealth codecompanion` to check that plugin is installed correctly. After
that, consider using the minimal.lua
<https://github.com/olimorris/codecompanion.nvim/blob/main/minimal.lua> file to
troubleshoot, running it with `nvim --clean -u minimal.lua`.


==============================================================================
3. Getting Started                             *codecompanion-getting-started*

Please see the author‚Äôs own config
<https://github.com/olimorris/dotfiles/blob/main/.config/nvim/lua/plugins/coding.lua>
for a complete reference of how to setup the plugin.


  [!IMPORTANT] The default adapter in CodeCompanion is GitHub Copilot
  <https://docs.github.com/en/copilot/using-github-copilot/copilot-chat/asking-github-copilot-questions-in-your-ide>.
  If you have copilot.vim <https://github.com/github/copilot.vim> or copilot.lua
  <https://github.com/zbirenbaum/copilot.lua> installed then expect CodeCompanion
  to work out of the box.

WITH AN ADAPTER                *codecompanion-getting-started-with-an-adapter*


  [!NOTE] The adapters that the plugin supports out of the box can be found here
  <https://github.com/olimorris/codecompanion.nvim/tree/main/lua/codecompanion/adapters>.
  Or, see the user contributed adapters
  |codecompanion-configuration-adapters.html-community-adapters|
An adapter is what connects Neovim to an LLM. It‚Äôs the interface that allows
data to be sent, received and processed. In order to use the plugin, you need
to make sure you‚Äôve configured an adapter first:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          adapter = "anthropic",
        },
        inline = {
          adapter = "anthropic",
        },
      },
    }),
<

In the example above, we‚Äôre using the Anthropic adapter for both the chat and
inline strategies. Refer to the
|codecompanion-configuration-adapters-changing-a-model| section to understand
how to change the default model.

Because most LLMs require an API key you‚Äôll need to share that with the
adapter. By default, adapters will look in your environment for a `*_API_KEY`
where `*` is the name of the adapter such as `ANTHROPIC` or `OPENAI`. However,
you can extend the adapter and change the API key like so:

>lua
    require("codecompanion").setup({
      adapters = {
        anthropic = function()
          return require("codecompanion.adapters").extend("anthropic", {
            env = {
              api_key = "MY_OTHER_ANTHROPIC_KEY"
            },
          })
        end,
      },
    }),
<

Having API keys in plain text in your shell is not always safe. Thanks to this
PR <https://github.com/olimorris/codecompanion.nvim/pull/24>, you can run
commands from within your config by prefixing them with `cmd:`. In the example
below, we‚Äôre using the 1Password CLI to read an OpenAI credential.

>lua
    require("codecompanion").setup({
      adapters = {
        openai = function()
          return require("codecompanion.adapters").extend("openai", {
            env = {
              api_key = "cmd:op read op://personal/OpenAI/credential --no-newline",
            },
          })
        end,
      },
    }),
<


  [!IMPORTANT] Please see the section on |codecompanion-configuration-adapters|
  for more information

CHAT BUFFER                        *codecompanion-getting-started-chat-buffer*

The Chat Buffer is where you can converse with an LLM from within Neovim. It
operates on a single response per turn, basis.

Run `:CodeCompanionChat` to open a chat buffer. Type your prompt and send it by
pressing `<C-s>` while in insert mode or `<CR>` in normal mode. Alternatively,
run `:CodeCompanionChat why are Lua and Neovim so perfect together?` to open
the chat buffer and send a prompt at the same time. Toggle the chat buffer with
`:CodeCompanionChat Toggle`.

You can add context from your code base by using `Variables` and `Slash
Commands` in the chat buffer.


  [!IMPORTANT] As of `v17.5.0`, variables and tools are now wrapped in curly
  braces, such as `#{buffer}` or `@{files}`

VARIABLES ~

`Variables`, accessed via `#`, contain data about the present state of Neovim:

- `buffer` - Shares the current buffer‚Äôs code. This can also receive |codecompanion-usage-chat-buffer-variables-buffer|
- `lsp` - Shares LSP information and code for the current buffer
- `viewport` - Shares the buffers and lines that you see in the Neovim viewport


  [!TIP] Use them in your prompt like: `What does the code in #{buffer} do?`,
  ensuring they‚Äôre wrapped in curly brackets

SLASH COMMANDS ~


  [!IMPORTANT] These have been designed to work with native Neovim completions
  alongside nvim-cmp and blink.cmp. To open the native completion menu use
  `<C-_>` in insert mode when in the chat buffer. Note: Slash commands should
  also work with coc.nvim.
`Slash commands`, accessed via `/`, run commands to insert additional context
into the chat buffer:

- `/buffer` - Insert open buffers
- `/fetch` - Insert URL contents
- `/file` - Insert a file
- `/quickfix` - Insert entries from the quickfix list
- `/help` - Insert content from help tags
- `/now` - Insert the current date and time
- `/symbols` - Insert symbols from a selected file
- `/terminal` - Insert terminal output


AGENTS / TOOLS ~

`Tools`, accessed via `@`, allow the LLM to function as an agent and carry out
actions:

- `cmd_runner` - The LLM will run shell commands (subject to approval)
- `create_file` - The LLM will create a file in the current working directory (subject to approval)
- `file_search` - The LLM can search for a file in the CWD
- `get_changed_files` - The LLM can get git diffs for any changed files in the CWD
- `grep_search` - The LLM can search within files in the CWD
- `insert_edit_into_file` - The LLM will edit code in a Neovim buffer or on the file system (subject to approval)
- `next_edit_suggestion` - The LLM can show the user where the next edit is
- `read_file` - The LLM can read a specific file
- `web_search` - The LLM can search the internet for information

Tools can also be grouped together, also accessible via `@` in the chat buffer:

- `files` - Contains the `create_file`, `file_search`, `get_changed_files`, `grep_search`, `insert_edit_into_file` and `read_file` tools


  [!TIP] Use them in your prompt like: `Can you use the @{grep_search} tool to
  find occurrences of "add_message"`, ensuring they‚Äôre wrapped in curly
  brackets

INLINE ASSISTANT              *codecompanion-getting-started-inline-assistant*


  [!NOTE] The diff provider in the video is mini.diff
  <https://github.com/echasnovski/mini.diff>
The inline assistant enables an LLM to write code directly into a Neovim
buffer.

Run `:CodeCompanion <your prompt>` to call the inline assistant. The assistant
will evaluate the prompt and either write code or open a chat buffer. You can
also make a visual selection and call the assistant. To send additional context
alongside your prompt, you can leverage
|codecompanion-usage-inline-assistant-variables| such as `:CodeCompanion
#{buffer} <your prompt>`:

- `buffer` - shares the contents of the current buffer
- `chat` - shares the LLM‚Äôs messages from the last chat buffer

For convenience, you can call prompts from the
|codecompanion-configuration-prompt-library| via the cmd line, such as
`:'<,'>CodeCompanion /explain`. The prompt library comes with the following
defaults:

- `/commit` - Generate a commit message
- `/explain` - Explain how selected code in a buffer works
- `/fix` - Fix the selected code
- `/lsp` - Explain the LSP diagnostics for the selected code
- `/tests` - Generate unit tests for selected code


COMMANDS                              *codecompanion-getting-started-commands*

Use CodeCompanion to create Neovim commands in command-line mode
(|Command-line|) via `:CodeCompanionCmd <your prompt>`.


ACTION PALETTE                  *codecompanion-getting-started-action-palette*

Run `:CodeCompanionActions` to open the action palette, which gives you access
to the plugin‚Äôs features, including your prompts from the
|codecompanion-configuration-prompt-library|.

By default the plugin uses `vim.ui.select`, however, you can change the
provider by altering the `display.action_palette.provider` config value to be
`telescope`, `mini_pick` or `snacks`. You can also call the Telescope extension
with `:Telescope codecompanion`.


  [!NOTE] Some actions and prompts will only be visible if you‚Äôre in `Visual
  mode`.

LIST OF COMMANDS              *codecompanion-getting-started-list-of-commands*

The plugin has four core commands:

- `CodeCompanion` - Open the inline assistant
- `CodeCompanionChat` - Open a chat buffer
- `CodeCompanionCmd` - Generate a command in the command-line
- `CodeCompanionActions` - Open the `Action Palette`

However, there are multiple options available:

- `CodeCompanion <prompt>` - Prompt the inline assistant
- `CodeCompanion <adapter> <prompt>` - Prompt the inline assistant with a specific adapter
- `CodeCompanion /<prompt library>` - Call an item from the |codecompanion-configuration-prompt-library|
- `CodeCompanionChat <prompt>` - Send a prompt to the LLM via a chat buffer
- `CodeCompanionChat <adapter>` - Open a chat buffer with a specific adapter
- `CodeCompanionChat Add` - Add visually selected chat to the current chat buffer
- `CodeCompanionChat RefreshCache` - Used to refresh conditional elements in the chat buffer
- `CodeCompanionChat Toggle` - Toggle a chat buffer


SUGGESTED WORKFLOW          *codecompanion-getting-started-suggested-workflow*

For an optimum plugin workflow, I recommend the following:

>lua
    vim.keymap.set({ "n", "v" }, "<C-a>", "<cmd>CodeCompanionActions<cr>", { noremap = true, silent = true })
    vim.keymap.set({ "n", "v" }, "<LocalLeader>a", "<cmd>CodeCompanionChat Toggle<cr>", { noremap = true, silent = true })
    vim.keymap.set("v", "ga", "<cmd>CodeCompanionChat Add<cr>", { noremap = true, silent = true })
    
    -- Expand 'cc' into 'CodeCompanion' in the command line
    vim.cmd([[cab cc CodeCompanion]])
<


  [!NOTE] You can also assign prompts from the library to specific mappings. See
  the |codecompanion-configuration-prompt-library-assigning-prompts-to-a-keymap|
  section for more information.

==============================================================================
4. Configuration                                 *codecompanion-configuration*


GENERAL                                  *codecompanion-configuration-general*

This section sets out how various elements of CodeCompanion‚Äôs config can be
changed. The examples are shown wrapped in a
`require("codecompanion").setup({})` block to work with all plugin managers.

However, if you‚Äôre using Lazy.nvim <https://github.com/folke/lazy.nvim>, you
can apply config changes in the `opts` table which is much cleaner:

>lua
    {
      "olimorris/codecompanion.nvim",
      dependencies = {
        "nvim-lua/plenary.nvim",
        "nvim-treesitter/nvim-treesitter",
      },
      opts = {
        strategies = {
          -- Change the default chat adapter
          chat = {
            adapter = "anthropic",
          },
        },
        opts = {
          -- Set debug logging
          log_level = "DEBUG",
        },
      },
    },
<

Of course, peruse the rest of this section for more configuration options.


ACTION PALETTE                    *codecompanion-configuration-action-palette*

The Action Palette holds plugin specific items like the ability to launch a
chat buffer and the currently open chat buffers alongside displaying the
prompts from the |codecompanion-prompt-library|.


LAYOUT ~


  [!NOTE] The Action Palette also supports Telescope.nvim
  <https://github.com/nvim-telescope/telescope.nvim>, fzf_lua
  <https://github.com/ibhagwan/fzf-lua>, mini.pick
  <https://github.com/echasnovski/mini.pick> and snacks.nvim
  <https://github.com/folke/snacks.nvim>
You can change the appearance of the chat buffer by changing the
`display.action_palette` table in your configuration:

>lua
    require("codecompanion").setup({
      display = {
        action_palette = {
          width = 95,
          height = 10,
          prompt = "Prompt ", -- Prompt used for interactive LLM calls
          provider = "default", -- Can be "default", "telescope", "fzf_lua", "mini_pick" or "snacks". If not specified, the plugin will autodetect installed providers.
          opts = {
            show_default_actions = true, -- Show the default actions in the action palette?
            show_default_prompt_library = true, -- Show the default prompt library in the action palette?
          },
        },
      },
    }),
<


ADAPTERS                                *codecompanion-configuration-adapters*


  [!TIP] Want to connect to an LLM that isn‚Äôt supported out of the box? Check
  out |codecompanion--community-adapters| user contributed adapters,
  |codecompanion-extending-adapters.html| your own or post in the discussions
  <https://github.com/olimorris/codecompanion.nvim/discussions>
An adapter is what connects Neovim to an LLM. It‚Äôs the interface that allows
data to be sent, received and processed and there are a multitude of ways to
customize them.


CHANGING THE DEFAULT ADAPTER ~

You can change the default adapter as follows:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          adapter = "anthropic",
        },
        inline = {
          adapter = "copilot",
        },
        cmd = {
          adapter = "deepseek",
        }
      },
    }),
<


SETTING AN API KEY ~

Extend a base adapter to set options like `api_key` or `model`:

>lua
    require("codecompanion").setup({
      adapters = {
        anthropic = function()
          return require("codecompanion.adapters").extend("anthropic", {
            env = {
              api_key = "MY_OTHER_ANTHROPIC_KEY",
            },
          })
        end,
      },
    }),
<

If you do not want to store secrets in plain text, prefix commands with `cmd:`:

>lua
    require("codecompanion").setup({
      adapters = {
        openai = function()
          return require("codecompanion.adapters").extend("openai", {
            env = {
              api_key = "cmd:op read op://personal/OpenAI/credential --no-newline",
            },
          })
        end,
      },
    }),
<


  [!NOTE] In this example, we‚Äôre using the 1Password CLI to extract the OpenAI
  API Key. You could also use gpg as outlined here
  <https://github.com/olimorris/codecompanion.nvim/discussions/601>
Environment variables can also be functions and as a parameter, they receive a
copy of the adapter itself.


CHANGING A MODEL ~

To more easily change a model associated with a strategy you can pass in the
`name` and `model` to the adapter:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          adapter = {
            name = "copilot",
            model = "claude-sonnet-4-20250514",
          },
        },
      },
    }),
<

To change the default model on an adapter you can modify the
`schema.model.default` property:

>lua
    require("codecompanion").setup({
      adapters = {
        openai = function()
          return require("codecompanion.adapters").extend("openai", {
            schema = {
              model = {
                default = "gpt-4.1",
              },
            },
          })
        end,
      },
    }),
<


ADAPTER SETTINGS ~

LLMs have many settings such as model, temperature and max_tokens. In an
adapter, these sit within a schema table and can be configured during setup:

>lua
    require("codecompanion").setup({
      adapters = {
        llama3 = function()
          return require("codecompanion.adapters").extend("ollama", {
            name = "llama3", -- Give this adapter a different name to differentiate it from the default ollama adapter
            schema = {
              model = {
                default = "llama3:latest",
              },
              num_ctx = {
                default = 16384,
              },
              num_predict = {
                default = -1,
              },
            },
          })
        end,
      },
    })
<


ADDING A CUSTOM ADAPTER ~


  [!NOTE] See the |codecompanion-extending-adapters| section to learn how to
  create custom adapters
Custom adapters can be added to the plugin as follows:

>lua
    require("codecompanion").setup({
      adapters = {
        my_custom_adapter = function()
          return {} -- My adapter logic
        end,
      },
    }),
<


SETTING A PROXY ~

A proxy can be configured by utilising the `adapters.opts` table in the config:

>lua
    require("codecompanion").setup({
      adapters = {
        opts = {
          allow_insecure = true,
          proxy = "socks5://127.0.0.1:9999",
        },
      },
    }),
<


COMMUNITY ADAPTERS ~

Thanks to the community for building the following adapters:

- Venice.ai <https://github.com/olimorris/codecompanion.nvim/discussions/972>
- Fireworks.ai <https://github.com/olimorris/codecompanion.nvim/discussions/693>
- OpenRouter <https://github.com/olimorris/codecompanion.nvim/discussions/1013>

The section of the discussion forums which is dedicated to user created
adapters can be found here
<https://github.com/olimorris/codecompanion.nvim/discussions?discussions_q=is%3Aopen+label%3A%22tip%3A+adapter%22>.
Use these individual threads as a place to raise issues and ask questions about
your specific adapters.


EXAMPLE: USING OPENAI COMPATIBLE MODELS ~

If your LLM states that it is `‚ÄúOpenAI compatible‚Äù`, then you can leverage
the `openai_compatible` adapter, modifying some elements such as the URL in the
env table, the API key and altering the schema:


  [!NOTE] The schema in this instance is provided only as an example and must be
  modified according to the requirements of the model you use. The options are
  chosen to show how to use different types of parameters.
>lua
    require("codecompanion").setup({
      adapters = {
        my_openai = function()
          return require("codecompanion.adapters").extend("openai_compatible", {
            env = {
              url = "http[s]://open_compatible_ai_url", -- optional: default value is ollama url http://127.0.0.1:11434
              api_key = "OpenAI_API_KEY", -- optional: if your endpoint is authenticated
              chat_url = "/v1/chat/completions", -- optional: default value, override if different
              models_endpoint = "/v1/models", -- optional: attaches to the end of the URL to form the endpoint to retrieve models
            },
            schema = {
              model = {
                default = "deepseek-r1-671b",  -- define llm model to be used
              },
              temperature = {
                order = 2,
                mapping = "parameters",
                type = "number",
                optional = true,
                default = 0.8,
                desc = "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.",
                validate = function(n)
                  return n >= 0 and n <= 2, "Must be between 0 and 2"
                end,
              },
              max_completion_tokens = {
                order = 3,
                mapping = "parameters",
                type = "integer",
                optional = true,
                default = nil,
                desc = "An upper bound for the number of tokens that can be generated for a completion.",
                validate = function(n)
                  return n > 0, "Must be greater than 0"
                end,
              },
              stop = {
                order = 4,
                mapping = "parameters",
                type = "string",
                optional = true,
                default = nil,
                desc = "Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile.",
                validate = function(s)
                  return s:len() > 0, "Cannot be an empty string"
                end,
              },
              logit_bias = {
                order = 5,
                mapping = "parameters",
                type = "map",
                optional = true,
                default = nil,
                desc = "Modify the likelihood of specified tokens appearing in the completion. Maps tokens (specified by their token ID) to an associated bias value from -100 to 100. Use https://platform.openai.com/tokenizer to find token IDs.",
                subtype_key = {
                  type = "integer",
                },
                subtype = {
                  type = "integer",
                  validate = function(n)
                    return n >= -100 and n <= 100, "Must be between -100 and 100"
                  end,
                },
              },
            },
          })
        end,
      },
    })
<


EXAMPLE: USING OLLAMA REMOTELY ~

To use Ollama remotely, change the URL in the env table, set an API key and
pass it via an "Authorization" header:

>lua
    require("codecompanion").setup({
      adapters = {
        ollama = function()
          return require("codecompanion.adapters").extend("ollama", {
            env = {
              url = "https://my_ollama_url",
              api_key = "OLLAMA_API_KEY",
            },
            headers = {
              ["Content-Type"] = "application/json",
              ["Authorization"] = "Bearer ${api_key}",
            },
            parameters = {
              sync = true,
            },
          })
        end,
      },
    })
<


EXAMPLE: AZURE OPENAI ~

Below is an example of how you can leverage the `azure_openai` adapter within
the plugin:

>lua
    require("codecompanion").setup({
      adapters = {
        azure_openai = function()
          return require("codecompanion.adapters").extend("azure_openai", {
            env = {
              api_key = "YOUR_AZURE_OPENAI_API_KEY",
              endpoint = "YOUR_AZURE_OPENAI_ENDPOINT",
            },
            schema = {
              model = {
                default = "YOUR_DEPLOYMENT_NAME",
              },
            },
          })
        end,
      },
      strategies = {
        chat = {
          adapter = "azure_openai",
        },
        inline = {
          adapter = "azure_openai",
        },
      },
    }),
<


HIDING DEFAULT ADAPTERS ~

By default, the plugin shows all available adapters, including the defaults. If
you prefer to only display the adapters defined in your user configuration, you
can set the `show_defaults` option to `false`:

>lua
    require("codecompanion").setup({
      adapters = {
        opts = {
          show_defaults = false,
        },
        -- Define your custom adapters here
      },
    })
<


CONTROLLING MODEL CHOICES ~

When switching between adapters, the plugin typically displays all available
model choices for the selected adapter. If you want to simplify the interface
and have the default model automatically chosen (without showing any model
selection UI), you can set the `show_model_choices` option to `false`:

>lua
    require("codecompanion").setup({
      adapters = {
        opts = {
          show_model_choices = false,
        },
        -- Define your custom adapters here
      },
    })
<

With `show_model_choices = false`, the default model (as defined in the
adapter‚Äôs schema) will be automatically selected when changing adapters, and
no model selection will be shown to the user.


CHAT BUFFER                          *codecompanion-configuration-chat-buffer*

By default, CodeCompanion provides a "chat" strategy that uses a dedicated
Neovim buffer for conversational interaction with your chosen LLM. This buffer
can be customized according to your preferences.

Please refer to the config.lua
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua#L42-L392>
file for a full list of all configuration options.


KEYMAPS ~


  [!NOTE] The plugin scopes CodeCompanion specific keymaps to the `chat buffer`
  only.
You can define or override the default keymaps
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua#L178>
to send messages, regenerate responses, close the buffer, etc. Example:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          keymaps = {
            send = {
              modes = { n = "<C-s>", i = "<C-s>" },
              opts = {},
            },
            close = {
              modes = { n = "<C-c>", i = "<C-c>" },
              opts = {},
            },
            -- Add further custom keymaps here
          },
        },
      },
    })
<

The keymaps are mapped to `<C-s>` for sending a message and `<C-c>` for closing
in both normal and insert modes. To set other `:map-arguments`, you can use the
optional `opts` table which will be fed to `vim.keymap.set`.


VARIABLES ~

Variables
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua#L90>
are placeholders inserted into the chat buffer (using `#`). They provide
contextual code or information about the current Neovim state. For instance,
the built-in `#buffer` variable sends the current buffer‚Äôs contents to the
LLM.

You can even define your own variables to share specific content:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          variables = {
            ["my_var"] = {
              ---Ensure the file matches the CodeCompanion.Variable class
              ---@return string|fun(): nil
              callback = "/Users/Oli/Code/my_var.lua",
              description = "Explain what my_var does",
              opts = {
                contains_code = false,
                --has_params = true,    -- Set this if your variable supports parameters
                --default_params = nil, -- Set default parameters
              },
            },
          },
        },
      },
    })
<


SLASH COMMANDS ~

Slash Commands
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua#L114>
(invoked with `/`) let you dynamically insert context into the chat buffer,
such as file contents or date/time.

The plugin supports providers like telescope
<https://github.com/nvim-telescope/telescope.nvim>, mini_pick
<https://github.com/echasnovski/mini.pick>, fzf_lua
<https://github.com/ibhagwan/fzf-lua> and snacks.nvim
<https://github.com/folke/snacks.nvim>. By default, the plugin will
automatically detect if you have any of those plugins installed and duly set
them as the default provider. Failing that, the in-built `default` provider
will be used. Please see the |codecompanion-usage-chat-buffer-index| usage
section for information on how to use Slash Commands.

You can configure Slash Commands with:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          slash_commands = {
            ["file"] = {
              -- Location to the slash command in CodeCompanion
              callback = "strategies.chat.slash_commands.file",
              description = "Select a file using Telescope",
              opts = {
                provider = "telescope", -- Can be "default", "telescope", "fzf_lua", "mini_pick" or "snacks"
                contains_code = true,
              },
            },
          },
        },
      },
    })
<


  [!IMPORTANT] Each slash command may have their own unique configuration so be
  sure to check out the config.lua
  <https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua>
  file
You can also add your own slash commands:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          slash_commands = {
            ["git_files"] = {
              description = "List git files",
              ---@param chat CodeCompanion.Chat
              callback = function(chat)
                local handle = io.popen("git ls-files")
                if handle ~= nil then
                  local result = handle:read("*a")
                  handle:close()
                  chat:add_reference({ role = "user", content = result }, "git", "<git_files>")
                else
                  return vim.notify("No git files available", vim.log.levels.INFO, { title = "CodeCompanion" })
                end
              end,
              opts = {
                contains_code = false,
              },
            },
          },
        },
      },
    })
<

Credit to @lazymaniac <https://github.com/lazymaniac> for the inspiration
<https://github.com/olimorris/codecompanion.nvim/discussions/958>.


  [!NOTE] You can also point the callback to a lua file that resides within your
  own configuration

KEYMAPS

Slash Commands can also be called via keymaps, in the chat buffer. Simply add a
`keymaps` table to the Slash Command you‚Äôd like to call. For example:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          slash_commands = {
            ["buffer"] = {
              keymaps = {
                modes = {
                  i = "<C-b>",
                  n = { "<C-b>", "gb" },
                },
              },
            },
          },
        },
      },
    })
<


AGENTS AND TOOLS ~

Tools
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua#L55>
perform specific tasks (e.g., running shell commands, editing buffers, etc.)
when invoked by an LLM. Multiple tools can be grouped together. Both can be
referenced with `@` when in the chat buffer:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          tools = {
            ["my_tool"] = {
              description = "Run a custom task",
              callback = require("user.codecompanion.tools.my_tool")
            },
            groups = {
              ["my_group"] = {
                description = "A custom agent combining tools",
                system_prompt = "Describe what the agent should do",
                tools = {
                  "cmd_runner",
                  "insert_edit_into_file",
                  -- Add your own tools or reuse existing ones
                },
                opts = {
                  collapse_tools = true, -- When true, show as a single group reference instead of individual tools
                },
              },
            },
          },
        },
      },
    })
<

When users introduce the group, `my_group`, in the chat buffer, it can call the
tools you listed (such as `cmd_runner`) to perform tasks on your code.

A tool is a |codecompanion-extending-tools| table with specific keys that
define the interface and workflow of the tool. The table can be resolved using
the `callback` option. The `callback` option can be a table itself or either a
function or a string that points to a luafile that return the table.


TOOL CONDITIONALS

Tools can also be conditionally enabled:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          tools = {
            ["grep_search"] = {
              ---@return boolean
              enabled = function()
                return vim.fn.executable("rg") == 1
              end,
            },
          }
        }
      }
    })
<

This is useful to ensure that a particular dependency is installed on the
machine. After the user has installed the dependency, the `:CodeCompanionChat
RefreshCache` command can be used to refresh the cache‚Äôs across chat buffers.


APPROVALS

Some tools, such as |codecompanion-usage-chat-buffer-agents.html-cmd-runner|,
require the user to approve any commands before they‚Äôre executed. This can be
changed by altering the config for each tool:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          tools = {
            ["cmd_runner"] = {
              opts = {
                requires_approval = false,
              },
            },
          }
        }
      }
    })
<

You can also force any tool to require your approval by adding in
`opts.requires_approval = true`.


AUTO SUBMIT TOOL OUTPUT (RECURSION)

When a tool executes, it can be useful to automatically send its output back to
the LLM. This can be achieved by the following options in your configuration:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          tools = {
            opts = {
              auto_submit_errors = true, -- Send any errors to the LLM automatically?
              auto_submit_success = true, -- Send any successful output to the LLM automatically?
            },
          }
        }
      }
    })
<


AUTOMATICALLY ADD TOOLS TO CHAT

You can configure the plugin to automatically add tools and tool groups to new
chat buffers:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          tools = {
            opts = {
              default_tools = {
                "my_tool",
                "my_tool_group"
              }
            },
          }
        }
      }
    })
<

This also works for |codecompanion-configuration-extensions|.


PROMPT DECORATOR ~

It can be useful to decorate your prompt, prior to sending to an LLM, with
additional information. For example, the GitHub Copilot prompt in VS Code,
wraps a user‚Äôs prompt between `<prompt></prompt>` tags presumably to
differentiate the user‚Äôs ask from additional context. This can also be
achieved in CodeCompanion:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          opts = {
            ---Decorate the user message before it's sent to the LLM
            ---@param message string
            ---@param adapter CodeCompanion.Adapter
            ---@param context table
            ---@return string
            prompt_decorator = function(message, adapter, context)
              return string.format([[<prompt>%s</prompt>]], message)
            end,
          }
        }
      }
    })
<

The decorator function also has access to the adapter in the chat buffer
alongside the context
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/utils/context.lua#L121-L137>
table (which refreshes when a user toggles the chat buffer).


LAYOUT ~

You can change the appearance
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua#L903>
of the chat buffer by changing the `display.chat.window` table in your
configuration:

>lua
    require("codecompanion").setup({
      display = {
        chat = {
          -- Change the default icons
          icons = {
            buffer_pin = "Ôêµ ",
            buffer_watch = "üëÄ ",
          },
    
          -- Alter the sizing of the debug window
          debug_window = {
            ---@return number|fun(): number
            width = vim.o.columns - 5,
            ---@return number|fun(): number
            height = vim.o.lines - 2,
          },
    
          -- Options to customize the UI of the chat buffer
          window = {
            layout = "vertical", -- float|vertical|horizontal|buffer
            position = nil, -- left|right|top|bottom (nil will default depending on vim.opt.splitright|vim.opt.splitbelow)
            border = "single",
            height = 0.8,
            width = 0.45,
            relative = "editor",
            full_height = true, -- when set to false, vsplit will be used to open the chat buffer vs. botright/topleft vsplit
            opts = {
              breakindent = true,
              cursorcolumn = false,
              cursorline = false,
              foldcolumn = "0",
              linebreak = true,
              list = false,
              numberwidth = 1,
              signcolumn = "no",
              spell = false,
              wrap = true,
            },
          },
    
          ---Customize how tokens are displayed
          ---@param tokens number
          ---@param adapter CodeCompanion.Adapter
          ---@return string
          token_count = function(tokens, adapter)
            return " (" .. tokens .. " tokens)"
          end,
        },
      },
    }),
<


DIFF ~


  [!NOTE] Currently the plugin only supports native Neovim diff or mini.diff
  <https://github.com/echasnovski/mini.diff>
If you utilize the `insert_edit_into_file` tool, then the plugin can update a
given chat buffer. A diff will be created so you can see the changes made by
the LLM.

There are a number of diff settings available to you:

>lua
    require("codecompanion").setup({
      display = {
        diff = {
          enabled = true,
          close_chat_at = 240, -- Close an open chat buffer if the total columns of your display are less than...
          layout = "vertical", -- vertical|horizontal split for default provider
          opts = { "internal", "filler", "closeoff", "algorithm:patience", "followwrap", "linematch:120" },
          provider = "default", -- default|mini_diff
        },
      },
    }),
<


USER INTERFACE (UI) ~


  [!NOTE] The |codecompanion-installation-additional-plugins| section contains
  installation instructions for some popular markdown rendering plugins

USER AND LLM ROLES

The chat buffer places user and LLM responses under a `H2` header. These can be
customized in the configuration:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          roles = {
            ---The header name for the LLM's messages
            ---@type string|fun(adapter: CodeCompanion.Adapter): string
            llm = function(adapter)
              return "CodeCompanion (" .. adapter.formatted_name .. ")"
            end,
    
            ---The header name for your messages
            ---@type string
            user = "Me",
          }
        }
      }
    })
<

By default, the LLM‚Äôs responses will be placed under a header such as
`CodeCompanion (DeepSeek)`, leveraging the current adapter in the chat buffer.
This option can be in the form of a string or a function that returns a string.
If you opt for a function, the first parameter will always be the adapter from
the chat buffer.

The user role is currently only available as a string.


COMPLETION

By default, CodeCompanion looks to use the fantastic blink.cmp
<https://github.com/Saghen/blink.cmp> plugin to complete variables, slash
commands and tools. However, you can override this in your config:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          opts = {
            completion_provider = "cmp", -- blink|cmp|coc|default
          }
        }
      }
    })
<

The plugin also supports nvim-cmp <https://github.com/hrsh7th/nvim-cmp>, a
native completion solution (`default`), and coc.nvim
<https://github.com/neoclide/coc.nvim>.


AUTO SCROLLING

By default, the page scrolls down automatically as the response streams, with
the cursor placed at the end. This can be distracting if you are focusing on
the earlier content while the page scrolls up away during a long response. You
can disable this behavior using a flag:

>lua
    require("codecompanion").setup({
      display = {
        chat = {
          auto_scroll = false
        },
      },
    }),
<


ADDITIONAL OPTIONS ~

There are also a number of other options that you can customize:

>lua
    require("codecompanion").setup({
      display = {
        chat = {
          intro_message = "Welcome to CodeCompanion ‚ú®! Press ? for options",
          show_header_separator = false, -- Show header separators in the chat buffer? Set this to false if you're using an external markdown formatting plugin
          separator = "‚îÄ", -- The separator between the different messages in the chat buffer
          show_references = true, -- Show references (from slash commands and variables) in the chat buffer?
          show_settings = false, -- Show LLM settings at the top of the chat buffer?
          show_token_count = true, -- Show the token count for each response?
          start_in_insert_mode = false, -- Open the chat buffer in insert mode?
        },
      },
    }),
<


JUMP ACTION ~

The jump action (the command/function triggered by the `gR` keymap) can be
customised as follows:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          opts = {
            goto_file_action = 'tabnew', -- this will always open the file in a new tab
          },
        },
      },
    })
<

This can either be a string (denoting a VimScript command), or a function that
takes a single parameter (the path to the file to jump to). The default action
is to jump to an existing tab if the file is already opened, and open a new tab
otherwise.


INLINE ASSISTANT                *codecompanion-configuration-inline-assistant*

CodeCompanion provides an `inline` strategy for quick, direct interaction with
your code. Unlike the chat buffer, the inline assistant integrates responses
directly into the current buffer‚Äîallowing the LLM to add or replace code as
needed.


KEYMAPS ~

The inline assistant supports keymaps for accepting or rejecting changes:

>lua
    require("codecompanion").setup({
      strategies = {
        inline = {
          keymaps = {
            accept_change = {
              modes = { n = "ga" },
              description = "Accept the suggested change",
            },
            reject_change = {
              modes = { n = "gr" },
              description = "Reject the suggested change",
            },
          },
        },
      },
    }),
<

In this example, `<leader>a` (or `ga` on some keyboards) accepts inline
changes, while `gr` rejects them.


VARIABLES ~

The plugin comes with a number of
|codecompanion-usage-inline-assistant.html-variables| that can be used
alongside your prompt using the `#{}` syntax (e.g., `#{my_new_var}`). You can
also add your own:

>lua
    require("codecompanion").setup({
      strategies = {
        inline = {
          variables = {
            ["my_new_var"] = {
              ---@return string
              callback = "/Users/Oli/Code/my_var.lua",
              description = "My shiny new variable",
              opts = {
                contains_code = true,
              },
            },
          }
        }
      }
    })
<


LAYOUT ~

If the inline prompt creates a new buffer, you can also customize if this
should be output in a vertical/horizontal split or a new buffer:

>lua
    require("codecompanion").setup({
      display = {
        inline = {
          layout = "vertical", -- vertical|horizontal|buffer
        },
      }
    }),
<


DIFF ~

Please see the |codecompanion-chat-buffer-diff| on the Chat Buffer page for
configuration options.


PROMPT LIBRARY                    *codecompanion-configuration-prompt-library*

The plugin comes with a number of pre-built prompts. As per the config
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua>,
these can be called via keymaps or via the cmdline. These prompts have been
carefully curated to mimic those in GitHub‚Äôs Copilot Chat
<https://docs.github.com/en/copilot/using-github-copilot/asking-github-copilot-questions-in-your-ide>.
Of course, you can create your own prompts and add them to the Action Palette
or even to the slash command completion menu in the chat buffer.


ADDING PROMPTS ~


  [!NOTE] See the |codecompanion-extending-prompts| guide to learn more on their
  syntax and how you can create your own
Custom prompts can be added as follows:

>lua
    require("codecompanion").setup({
      prompt_library = {
        ["Docusaurus"] = {
          strategy = "chat",
          description = "Write documentation for me",
          opts = {
            index = 11,
            is_slash_cmd = false,
            auto_submit = false,
            short_name = "docs",
          },
          references = {
            {
              type = "file",
              path = {
                "doc/.vitepress/config.mjs",
                "lua/codecompanion/config.lua",
                "README.md",
              },
            },
          },
          prompts = {
            {
              role = "user",
              content = [[I'm rewriting the documentation for my plugin CodeCompanion.nvim, as I'm moving to a vitepress website. Can you help me rewrite it?
    
    I'm sharing my vitepress config file so you have the context of how the documentation website is structured in the `sidebar` section of that file.
    
    I'm also sharing my `config.lua` file which I'm mapping to the `configuration` section of the sidebar.
    ]],
            },
          },
        },
      },
    })
<


ASSIGNING PROMPTS TO A KEYMAP ~

You can assign prompts from the prompt library to a keymap via the `prompt`
function:

>lua
    vim.keymap.set("n", "<LocalLeader>d", function()
      require("codecompanion").prompt("docs")
    end, { noremap = true, silent = true })
<

Where `docs` is the `short_name` of the prompt.


SYSTEM PROMPT                      *codecompanion-configuration-system-prompt*

The default system prompt has been carefully curated to deliver terse and
professional responses that relate to development and Neovim. It is sent with
every request in the chat buffer.

The plugin comes with the following system prompt:

>txt
    You are an AI programming assistant named "CodeCompanion". You are currently plugged in to the Neovim text editor on a user's machine.
    
    Your core tasks include:
    - Answering general programming questions.
    - Explaining how the code in a Neovim buffer works.
    - Reviewing the selected code in a Neovim buffer.
    - Generating unit tests for the selected code.
    - Proposing fixes for problems in the selected code.
    - Scaffolding code for a new workspace.
    - Finding relevant code to the user's query.
    - Proposing fixes for test failures.
    - Answering questions about Neovim.
    - Running tools.
    
    You must:
    - Follow the user's requirements carefully and to the letter.
    - Keep your answers short and impersonal, especially if the user responds with context outside of your tasks.
    - Minimize other prose.
    - Use Markdown formatting in your answers.
    - Include the programming language name at the start of the Markdown code blocks.
    - Avoid including line numbers in code blocks.
    - Avoid wrapping the whole response in triple backticks.
    - Only return code that's relevant to the task at hand. You may not need to return all of the code that the user has shared.
    - Use actual line breaks instead of '\n' in your response to begin new lines.
    - Use '\n' only when you want a literal backslash followed by a character 'n'.
    - All non-code responses must be in %s.
    
    When given a task:
    1. Think step-by-step and describe your plan for what to build in pseudocode, written out in great detail, unless asked not to do so.
    2. Output the code in a single code block, being careful to only return relevant code.
    3. You should always generate short suggestions for the next user turns that are relevant to the conversation.
    4. You can only give one reply for each conversation turn.
<


CHANGING THE SYSTEM PROMPT ~

The default system prompt can be changed with:

>lua
    require("codecompanion").setup({
      opts = {
        system_prompt = function(opts)
          return "My new system prompt"
        end,
      },
    }),
<

The `opts` parameter contains the default adapter for the chat strategy
(`opts.adapter`) alongside the language (`opts.language`) that the LLM should
respond with.


EXTENSIONS                            *codecompanion-configuration-extensions*

CodeCompanion supports extensions similar to telescope.nvim, allowing users to
create functionality that can be shared with others. Extensions can either be
distributed as plugins or defined locally in your configuration.


INSTALLING EXTENSIONS ~

CodeCompanion supports extensions that add additional functionality to the
plugin. For example, to install and set up the mcphub extension using
lazy.nvim:

1. Install the extension:

>lua
    {
      "olimorris/codecompanion.nvim",
      dependencies = {
        -- Add mcphub.nvim as a dependency
        "ravitemer/mcphub.nvim" 
      }
    }
<

1. Add extension to your config with additional options:

>lua
    -- Configure in your setup
    require("codecompanion").setup({
      extensions = {
        mcphub = {
          callback = "mcphub.extensions.codecompanion",
          opts = {
            make_vars = true,       
            make_slash_commands = true,
            show_result_in_chat = true  
          }
        }
      }
    })
<

Visit the creating |codecompanion-extending-extensions| guide to learn more
about available extensions and how to create your own.


OTHER OPTIONS                      *codecompanion-configuration-other-options*


LOG LEVEL ~


  [!IMPORTANT] By default, logs are stored at
  `~/.local/state/nvim/codecompanion.log`
When it comes to debugging, you can change the level of logging which takes
place in the plugin as follows:

>lua
    require("codecompanion").setup({
      opts = {
        log_level = "ERROR", -- TRACE|DEBUG|ERROR|INFO
      },
    }),
<


DEFAULT LANGUAGE ~

If you use the default system prompt, you can specify which language an LLM
should respond in by changing the `opts.language` option:

>lua
    require("codecompanion").setup({
      opts = {
        language = "English",
      },
    }),
<

Of course, if you have your own system prompt you can specify your own language
for the LLM to respond in.


SENDING CODE ~


  [!IMPORTANT] Whilst the plugin makes every attempt to prevent code from being
  sent to the LLM, use this option at your own risk
You can prevent any code from being sent to the LLM with:

>lua
    require("codecompanion").setup({
      opts = {
        send_code = false,
      },
    }),
<


HIGHLIGHT GROUPS ~

The plugin sets the following highlight groups during setup:

- `CodeCompanionChatInfo` - Information messages in the chat buffer
- `CodeCompanionChatError` - Error messages in the chat buffer
- `CodeCompanionChatWarn` - Warning messages in the chat buffer
- `CodeCompanionChatSubtext` - Messages that appear under the information, error or warning messages in the chat buffer
- `CodeCompanionChatHeader` - The headers in the chat buffer
- `CodeCompanionChatSeparator` - Separator between headings in the chat buffer
- `CodeCompanionChatTokens` - Virtual text in the chat buffer showing the token count
- `CodeCompanionChatTool` - Tools in the chat buffer
- `CodeCompanionChatToolGroups` - Tool groups in the chat buffer
- `CodeCompanionChatVariable` - Variables in the chat buffer
- `CodeCompanionVirtualText` - All other virtual text in the plugin


==============================================================================
5. Usage                                                 *codecompanion-usage*


GENERAL                                          *codecompanion-usage-general*

CodeCompanion continues to evolve with regular frequency. This page will
endeavour to serve as focal point for providing useful productivity tips for
the plugin.


COPYING CODE FROM A CHAT BUFFER ~

The fastest way to copy an LLM‚Äôs code output is with `gy`. This will yank the
nearest codeblock.


APPLYING AN LLM¬ÄÔøΩS EDITS TO A BUFFER OR FILE ~

The |codecompanion-usage-chat-buffer-agents-files| tool, combined with the
|codecompanion-usage-chat-buffer-variables.html-buffer| variable or
|codecompanion-usage-chat-buffer-slash-commands.html-buffer| slash command,
enables an LLM to modify code in a Neovim buffer. This is especially useful if
you do not wish to manually apply an LLM‚Äôs suggestions yourself. Simply tag
it in the chat buffer with `@files` or `@insert_edit_into_file`.


RUN TESTS FROM THE CHAT BUFFER ~

The |codecompanion-usage-chat-buffer-agents-cmd-runner| tool enables an LLM to
execute commands on your machine. This can be useful if you wish the LLM to run
a test suite on your behalf and give insight on failing cases. Simply tag the
`@cmd_runner` in the chat buffer and ask it run your tests.


NAVIGATING BETWEEN RESPONSES IN THE CHAT BUFFER ~

You can quickly move between responses in the chat buffer using `[[` or `]]`.


QUICKLY ACCESSING A CHAT BUFFER ~

The `:CodeCompanionChat Toggle` command will automatically create a chat buffer
if one doesn‚Äôt exist, open the last chat buffer or hide the current chat
buffer.

When in a chat buffer, you can cycle between other chat buffers with `{` or
`}`.


ACTION PALETTE                            *codecompanion-usage-action-palette*

The `Action Palette` has been designed to be your entry point for the many
configuration options that CodeCompanion offers. It can be opened with
`:CodeCompanionActions`.

Once opened, the user can see plugin defined actions such as `Chat` and `Open
Chats`. The latter, enabling the user to move between any open chat buffers.
These can be turned off in the config by setting
`display.action_palette.opts.show_default_actions = false`.


DEFAULT PROMPTS ~

The plugin also defines a number of prompts in the form of the prompt library:

- `Explain` - Explain how code in a buffer works
- `Fix Code` - Fix the selected code
- `Explain LSP Diagnostics` - Explain the LSP diagnostics for the selected code
- `Unit Tests` - Generate unit tests for selected code
- `Generate a Commit Message` - Generate a commit message
- `Workspace File` - Generating a new workspace file and/or creating a group


  [!INFO] These can also be called via the cmd line for example `:CodeCompanion
  /explain`
The plugin also contains an example workflow, `Code Workflow`. See the
|codecompanion-usage-workflows| for more information.

The default prompts can be turned off by setting
`display.action_palette.show_default_prompt_library = false`.


CHAT BUFFER                                  *codecompanion-usage-chat-buffer*


  [!NOTE] The chat buffer has a filetype of `codecompanion` and a buftype of
  `nofile`
You can open a chat buffer with the `:CodeCompanionChat` command or with
`require("codecompanion").chat()`. You can toggle the visibility of the chat
buffer with `:CodeCompanionChat Toggle` or `require("codecompanion").toggle()`.

The chat buffer uses markdown as its syntax and `H2` headers separate the user
and LLM‚Äôs responses. The plugin is turn-based, meaning that the user sends a
response which is then followed by the LLM‚Äôs. The user‚Äôs responses are
parsed by nvim-treesitter <https://github.com/nvim-treesitter/nvim-treesitter>
and sent via an adapter to an LLM for a response which is then streamed back
into the buffer. A response is sent to the LLM by pressing `<CR>` or `<C-s>`.
This can of course be changed as per the |codecompanion--keymaps| section.


MESSAGES ~


  [!TIP] The message history and adapter settings can be modified via the debug
  window (`gd`) in the chat buffer
It‚Äôs important to note that some messages, such as system prompts or context
provided via |codecompanion-usage-chat-buffer-slash-commands|, will be hidden.
This is to keep the chat buffer uncluttered from a UI perspective. Using the
`gd` keymap opens up the debug window, which allows the user to see the full
contents of the messages table which will be sent to the LLM on the next turn.

The message history cannot be altered directly in the chat buffer. However, it
can be modified in the debug window. This window is simply a Lua buffer which
the user can edit as they wish. To persist any changes, the chat buffer keymaps
for sending a message (defaults: `<CR>` or `<C-s>`) can be used.


IMAGES / VISION ~

Many LLMs have the ability to receive images as input (sometimes referred to as
vision). CodeCompanion supports the adding of images into the chat buffer via
the |codecompanion-usage-chat-buffer-slash-commands-image| slash command and
through the system clipboard with |codecompanion-installation-img-clip-nvim|.
CodeCompanion can work with images in your file system and also with remote
URLs, encoding both into a base64 representation.

If your adapter and model doesn‚Äôt support images, then CodeCompanion will
endeavour to ensure that the image is not included in the messages payload
that‚Äôs sent to the LLM.


REFERENCES / CONTEXT ~



Sharing context with an LLM is crucial in order to generate useful responses.
In the plugin, references are defined as output that is shared with a chat
buffer via a `Variable`, `Slash Command` or `Agent/Tool`. They appear in a
blockquote entitled `Context`. In essence, this is context that you‚Äôre
sharing with an LLM.


  [!IMPORTANT] References contain the data of an object at a point in time. By
  default, they **are not** self-updating
In order to allow for references to self-update, they can be `pinned` (for
files and buffers) or `watched` (for buffers).

File and buffer references can be `pinned` to a chat buffer with the `gp`
keymap (when your cursor is on the line of the shared buffer in the ‚Äú>
Context section). Pinning results in the content from the object being reloaded
and shared with the LLM on every turn. The advantage of this is that the LLM
will always receive a fresh copy of the source data regardless of any changes.
This can be useful if you‚Äôre working with agents and tools. However, please
note that this can consume a lot of tokens.

Buffer references can be `watched` via the `gw` keymap (when your cursor is on
the line of the shared buffer in the ‚Äú> Context section). Watching, whilst
similar to pinning, is a more token-conscious way of keeping the LLM up to date
on the contents of a buffer. Watchers track changes (adds, edits, deletes) in
the underlying buffer and update the LLM on each turn, with only those changes.

If a reference is added by mistake, it can be removed from the chat buffer by
simply deleting it from the `Context` blockquote. On the next turn, all context
related to that reference will be removed from the message history.

Finally, it‚Äôs important to note that all LLM endpoints require the sending of
previous messages that make up the conversation. So even though you‚Äôve shared
a reference once, many messages ago, the LLM will always have that context to
refer to.


SETTINGS ~



When conversing with an LLM, it can be useful to tweak model settings in
between responses in order to generate the perfect output. If settings are
enabled (`display.chat.show_settings = true`), then a yaml block will be
present at the top of the chat buffer which can be modified in between
responses. The yaml block is simply a representation of an adapter‚Äôs schema
table.


COMPLETION ~


  [!IMPORTANT] As of `v17.5.0`, variables and tools are wrapped in curly braces
  automatically, such as `#{buffer}` or `@{files}`


You can invoke the completion plugins by typing `#` or `@` followed by the
variable or tool name, which will trigger the completion menu. If you don‚Äôt
use a completion plugin, you can use native completions with no setup, invoking
them with `<C-_>` from within the chat buffer.


KEYMAPS ~

The plugin has a host of keymaps available in the chat buffer. Pressing `?` in
the chat buffer will conveniently display all of them to you.

The keymaps available to the user in normal mode are:

- `<CR>|<C-s>` to send a message to the LLM
- `<C-c>` to close the chat buffer
- `q` to stop the current request
- `ga` to change the adapter for the currentchat
- `gc` to insert a codeblock in the chat buffer
- `gd` to view/debug the chat buffer‚Äôs contents
- `gf` to fold any codeblocks in the chat buffer
- `gp` to pin a reference to the chat buffer
- `gw` to watch a referenced buffer
- `gr` to regenerate the last response
- `gR` to go to the file under cursor. If the file is already opened, it‚Äôll jump
    to the existing window. Otherwise, it‚Äôll be opened in a new tab.
- `gs` to toggle the system prompt on/off
- `gS` to show copilot usage stats
- `gta` to toggle auto tool mode
- `gx` to clear the chat buffer‚Äôs contents
- `gy` to yank the last codeblock in the chat buffer
- `[[` to move to the previous header
- `]]` to move to the next header
- `{` to move to the previous chat
- `}` to move to the next chat


AGENTS AND TOOLS                        *codecompanion-usage-agents-and-tools*


  [!IMPORTANT] As of `v17.5.0`, tools must be wrapped in curly braces, such as
  `@{grep_search}` or `@{files}`

  [!IMPORTANT] Not all LLMs support function calling and the use of tools. Please
  see the |codecompanion--compatibility| section for more information.
As outlined by Andrew Ng in Agentic Design Patterns Part 3, Tool Use
<https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use>,
LLMs can act as agents by leveraging external tools. Andrew notes some common
examples such as web searching or code execution that have obvious benefits
when using LLMs.

In the plugin, tools are simply context and actions that are shared with an LLM
via a `system` prompt. The LLM can act as an agent by requesting tools via the
chat buffer which in turn orchestrates their use within Neovim. Agents and
tools can be added as a participant to the chat buffer by using the `@` key.


  [!IMPORTANT] The agentic use of some tools in the plugin results in you, the
  developer, acting as the human-in-the-loop and approving their use.

HOW TOOLS WORK ~

Tools make use of an LLM‚Äôs function calling
<https://platform.openai.com/docs/guides/function-calling> ability. All tools
in CodeCompanion follow OpenAI‚Äôs function calling specification, here
<https://platform.openai.com/docs/guides/function-calling#defining-functions>.

When a tool is added to the chat buffer, the LLM is instructured by the plugin
to return a structured JSON schema which has been defined for each tool. The
chat buffer parses the LLMs response and detects the tool use before triggering
the `agent/init.lua` file. The agent triggers off a series of events, which
sees tool‚Äôs added to a queue and sequentially worked with their output being
shared back to the LLM via the chat buffer. Depending on the tool, flags may be
inserted on the chat buffer for later processing.

An outline of the architecture can be seen
|codecompanion-extending-tools-architecture|.


COMMUNITY TOOLS ~

There is also a thriving ecosystem of user created tools:

- VectorCode <https://github.com/Davidyz/VectorCode/tree/main> - A code repository indexing tool to supercharge your LLM experience
- mcphub.nvim <https://github.com/ravitemer/mcphub.nvim> - A powerful Neovim plugin for managing MCP (Model Context Protocol) servers

The section of the discussion forums which is dedicated to user created tools
can be found here
<https://github.com/olimorris/codecompanion.nvim/discussions/categories/tools>.


CMD_RUNNER ~

The `@cmd_runner` tool enables an LLM to execute commands on your machine,
subject to your authorization. For example:

>md
    Can you use the @{cmd_runner} tool to run my test suite with `pytest`?
<

>md
    Use the @{cmd_runner} tool to install any missing libraries in my project
<

Some commands do not write any data to stdout
<https://en.wikipedia.org/wiki/Standard_streams#Standard_output_(stdout)> which
means the plugin can‚Äôt pass the output of the execution to the LLM. When this
occurs, the tool will instead share the exit code.

The LLM is specifically instructed to detect if you‚Äôre running a test suite,
and if so, to insert a flag in its request. This is then detected and the
outcome of the test is stored in the corresponding flag on the chat buffer.
This makes it ideal for |codecompanion-extending-workflows| to hook into.

**Options:** - `requires_approval` require approval before running a command?
(Default: true)


CREATE_FILE ~


  [!NOTE] By default, this tool requires user approval before it can be executed
Create a file within the current working directory:

>md
    Can you create some test fixtures using the @{create_file} tool?
<

**Options:** - `requires_approval` require approval before creating a file?
(Default: true)


FILE_SEARCH ~

This tool enables an LLM to search for files in the current working directory
by glob pattern. It will return a list of relative paths for any matching
files.

>md
    Use the @{file_search} tool to list all the lua files in my project
<

**Options:** - `max_results` limits the amount of results that can be sent to
the LLM in the response (Default: 500)


GET_CHANGED_FILES ~

This tool enables an LLM to get git diffs of any file changes in the current
working directory. It will return a diff which can contain `staged`, `unstaged`
and `merge-conflicts`.

>md
    Use the @{get_changed_files} tool see what's changed
<

**Options:** - `max_lines` limits the amount of lines that can be sent to the
LLM in the response (Default: 1000)


GREP_SEARCH ~


  [!IMPORTANT] This tool requires ripgrep <https://github.com/BurntSushi/ripgrep>
  to be installed
This tool enables an LLM to search for text, within files, in the current
working directory. For every match, the output (`{filename}:{line number}
{relative filepath}`) will be shared with the LLM:

>md
    Use the @{grep_search} tool to find all occurrences of `buf_add_message`?
<

**Options:** - `max_files` (number) limits the amount of files that can be sent
to the LLM in the response (Default: 100) - `respect_gitignore` (boolean)
(Default: true)


INSERT_EDIT_INTO_FILE ~


  [!NOTE] By default, when editing files, this tool requires user approval before
  it can be executed
This tool can edit buffers and files for code changes from an LLM:

>md
    Use the @{insert_edit_into_file} tool to refactor the code in #buffer
<

>md
    Can you apply the suggested changes to the buffer with the @{insert_edit_into_file} tool?
<

**Options:** - `patching_algorithm` (string|table|function) The algorithm to
use to determine how to edit files and buffers - `requires_approval.buffer`
(boolean) Require approval before editng a buffer? (Default: false) -
`requires_approval.file` (boolean) Require approval before editng a file?
(Default: true) - `user_confirmation` (boolean) require confirmation from the
user before moving on in the chat buffer? (Default: true)


NEXT_EDIT_SUGGESTION ~

Inspired by Copilot Next Edit Suggestion
<https://code.visualstudio.com/blogs/2025/02/12/next-edit-suggestions>, the
tool gives the LLM the ability to show the user where the next edit is. The LLM
can only suggest edits in files or buffers that have been shared with it as
context.

**Options:** - `jump_action` (string|function) Determines how a jump to the
next edit is made (Default: `tabnew`)


READ_FILE ~

This tool can read the contents of a specific file in the current working
directory. This can be useful for an LLM to gain wider context of files that
haven‚Äôt been shared with it.


WEB_SEARCH ~

The `@web_search` tool enables an LLM to search the web for a specific query.
This can be useful to supplement an LLMs knowledge cut off date with more up to
date information.

>md
    Can you use the @{web_search} tool to tell me the latest version of Neovim?
<

Currently, the tool uses tavily <https://www.tavily.com> and you‚Äôll need to
ensure that an API key has been set accordingly, as per the adapter
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/adapters/tavily.lua>.

You can also ask it to search under a specific domain:

>
    Using the @web_search tool to search from `https://neovim.io` and explain how I can configure a new language server.
<


TOOL GROUPS ~

CodeCompanion comes with two built-in tool groups:

- `full_stack_dev` - Containing all of the tools
- `files` - Containing `create_file`, `file_search`, `get_changed_files`, `grep_search`, `insert_edit_into_file` and `read_file` tools

When you include a tool group in your chat (e.g., `@{files}`), all tools within
that group become available to the LLM. By default, all the tools in the group
will be shown as a single `<group>name</group>` reference in the chat buffer.

If you want to show all tools as references in the chat buffer, set the
`opts.collapse_tools` option to `false` on the group itself.


APPROVALS ~

Some tools, such as the `@cmd_runner`, require the user to approve any actions
before they can be executed. If the tool requires this a `vim.fn.confirm`
dialog will prompt you for a response.


USEFUL TIPS ~


COMBINING TOOLS

Consider combining tools for complex tasks:

>md
    @{full_stack_dev} I want to play Snake. Can you create the game for me in Python and install any packages you need. Let's save it to ~/Code/Snake. When you've finished writing it, can you open it so I can play?
<


AUTOMATIC TOOL MODE

The plugin allows you to run tools on autopilot. This automatically approves
any tool use instead of prompting the user, disables any diffs, submits errors
and success messages and automatically saves any buffers that the agent has
edited. Simply set the global variable `vim.g.codecompanion_auto_tool_mode` to
enable this or set it to `nil` to undo this. Alternatively, the keymap `gta`
will toggle the feature whist from the chat buffer.


COMPATIBILITY ~

Below is the tool use status of various adapters and models in CodeCompanion:

  ------------------------------------------------------------------------
  Adapter        Model            Supported   Notes
  -------------- -------------- ------------- ----------------------------
  Anthropic                                   Dependent on the model

  Azure OpenAI                                Dependent on the model

  Copilot                                     Dependent on the model

  DeepSeek                                    Dependent on the model

  Gemini                                      Dependent on the model

  GitHub Models  All                          Not supported yet

  Huggingface    All                          Not supported yet

  Mistral        All                          Not supported yet

  Novita         All                          Not supported yet

  Ollama         Tested with                  Dependent on the model
                 Qwen3                        

  OpenAI                                      Dependent on the model and
  Compatible                                  provider

  OpenAI                                      Dependent on the model

  xAI            All                          Not supported yet
  ------------------------------------------------------------------------

SLASH COMMANDS                            *codecompanion-usage-slash-commands*

Slash Commands enable you to quickly add context to the chat buffer. They are
comprised of values present in the `strategies.chat.slash_commands` table
alongside the `prompt_library` table where individual prompts have
`opts.is_slash_cmd = true`.


/BUFFER ~


  [!NOTE] As of v16.2.0
  <https://github.com/olimorris/codecompanion.nvim/releases/tag/v16.2.0>, buffers
  are now watched by default
The `buffer` slash command enables you to add the contents of any open buffers
in Neovim to the chat buffer. The command has native, `Telescope`, `mini.pick`,
`fzf.lua` and `snacks.nvim` providers available. Also, multiple buffers can be
selected and added to the chat buffer as per the video above.


/FETCH ~


  [!TIP] To better understand a Neovim plugin, send its `config.lua` to your LLM
  via the `fetch` command alongside a prompt
The `fetch` slash command allows you to add the contents of a URL to the chat
buffer. By default, the plugin uses the awesome and powerful jina.ai
<https://jina.ai> to parse the page‚Äôs content and convert it into plain text.
For convenience, the slash command will cache the output to disk and prompt the
user if they wish to restore from the cache, should they look to fetch the same
URL.


/FILE ~

The `file` slash command allows you to add the contents of a file in the
current working directory to the chat buffer. The command has native,
`Telescope`, `mini.pick`, `fzf.lua` and `snacks.nvim` providers available.
Also, multiple files can be selected and added to the chat buffer:

- Select a single file: `‚èé enter`
- Select multiple files: `‚á• tab`

Please note that these mappings may be different depending on your provider.


/QUICKFIX ~

The `quickfix` slash command adds entries from the Neovim quickfix list to the
chat buffer.

- For search patterns or file entries, the whole file is shared.
- For diagnostics, the context of the function/method/class is shared if possible; otherwise, 10 lines around the diagnostic are included.


/HELP ~

The `help` slash command allows you to add content from a vim help file
(|helpfile|), to the chat buffer, by searching for help tags. Currently this is
only available for `Telescope`, `mini.pick`, `fzf_lua` and `snacks.nvim`
providers. By default, the slash command will prompt you to trim a help file
that is over 1,000 lines in length.


/IMAGE ~

The `image` slash command allows you to add images into a chat buffer via
remote URLs and through your file system. In the config for the slash command,
you can specify a group of directories (with `opts.dirs`) that the image picker
will always search in, alongside the current working directory. Currently the
image picker is only available with `snacks.nvim` and the `vim.ui.select`.


/NOW ~

The `now` slash command simply inserts the current datetime stamp into the chat
buffer.


/SYMBOLS ~


  [!NOTE] If a filetype isn‚Äôt supported please consider making a PR to add the
  corresponding Tree-sitter queries from aerial.nvim
  <https://github.com/stevearc/aerial.nvim>
The `symbols` slash command uses Tree-sitter to create a symbolic outline of a
file to share with the LLM. This can be a useful way to minimize token
consumption whilst sharing the basic outline of a file. The plugin utilizes the
amazing work from **aerial.nvim** by using their Tree-sitter symbol queries as
the basis. The list of filetypes that the plugin currently supports can be
found here <https://github.com/olimorris/codecompanion.nvim/tree/main/queries>.

The command has native, `Telescope`, `mini.pick`, `fzf.lua` and `snacks.nvim`
providers available. Also, multiple symbols can be selected and added to the
chat buffer.


/TERMINAL ~

The `terminal` slash command shares the latest output from the last terminal
buffer with the chat buffer. This can be useful for sharing the outputs of test
runs with your LLM.


/WORKSPACE ~

The `workspace` slash command allows users to share defined groups of files
and/or symbols with an LLM, alongside some pre-written context. The slash
command uses a codecompanion-workspace.json
<https://github.com/olimorris/codecompanion.nvim/blob/main/codecompanion-workspace.json>
file, stored in the current working directory, to house this context. It is, in
essence, a context management system for your repository.

Whilst LLMs are incredibly powerful, they have no knowledge of the
architectural decisions yourself or your team have made on a project. They have
no context as to why you‚Äôve selected the dependencies that you have. And,
they can‚Äôt see how your codebase has evolved over time. To help you create
your own workspace file, leverage the
|codecompanion-usage-action-palette.html-default-prompts| prompt in the action
palette and install the amazing VectorCode
<https://github.com/Davidyz/VectorCode/tree/main> tool.

Please see the |codecompanion-extending-workspace| guide to learn how to build
your own.


VARIABLES                                      *codecompanion-usage-variables*


  [!IMPORTANT] As of `v17.5.0`, variables must be wrapped in curly braces, such
  as `#{buffer}` or `#{lsp}`
Variables allow you to share data about the current state of Neovim with an
LLM. Simply type `#` in the chat buffer and trigger code completion if you‚Äôre
not using blink.cmp or nvim-cmp (or coc.nvim). Alternatively, type the
variables manually. After the response is sent to the LLM, you should see the
variable output tagged as a reference in the chat buffer.

Custom variables can be shared by adding them to the
`strategies.chat.variables` table in your configuration.


#BUFFER ~


  [!NOTE] As of v16.2.0
  <https://github.com/olimorris/codecompanion.nvim/releases/tag/v16.2.0>, buffers
  are now watched by default
The `#{buffer}` variable shares the full contents from the buffer that the user
was last in when they initiated `:CodeCompanionChat`. To select another buffer,
use the `/buffer` slash command. These buffers can be
|codecompanion-usage-chat-buffer-index-references| to enable updated content to
be automatically shared with the LLM:

- `#{buffer}{pin}` - To pin the current buffer
- `#{buffer}{watch}` - To watch the current buffer

To pin or watch buffers by default, you can add this configuration:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          variables = {
            ["buffer"] = {
              opts = {
                default_params = 'pin', -- or 'watch'
              },
            },
          },
        },
      },
    })
<


#LSP ~


  [!TIP] The |codecompanion-usage-action-palette| has a pre-built prompt which
  asks an LLM to explain LSP diagnostics in a visual selection
The `lsp` variable shares any information from the LSP servers that active in
the current buffer. This can serve as useful context should you wish to
troubleshoot any errors with an LLM.


#VIEWPORT ~

The `viewport` variable shares with the LLM, exactly what you see on your
screen at the point a response is sent (excluding the chat buffer of course).


EVENTS / HOOKS                            *codecompanion-usage-events-/-hooks*

In order to enable a tighter integration between CodeCompanion and your Neovim
config, the plugin fires events at various points during its lifecycle.


LIST OF EVENTS ~

The events that are fired from within the plugin are:

- `CodeCompanionChatCreated` - Fired after a chat has been created for the first time
- `CodeCompanionChatOpened` - Fired after a chat has been opened
- `CodeCompanionChatHidden` - Fired after a chat has been hidden
- `CodeCompanionChatClosed` - Fired after a chat has been permanently closed
- `CodeCompanionChatSubmitted` - Fired after a chat has been submitted
- `CodeCompanionChatStopped` - Fired after a chat has been stopped
- `CodeCompanionChatCleared` - Fired after a chat has been cleared
- `CodeCompanionChatAdapter` - Fired after the adapter has been set in the chat
- `CodeCompanionChatModel` - Fired after the model has been set in the chat
- `CodeCompanionChatPin` - Fired after a pinned reference has been updated in the messages table
- `CodeCompanionAgentStarted` - Fired when an agent has been initiated to run tools
- `CodeCompanionAgentFinished` - Fired when an agent has finished running all tools
- `CodeCompanionToolAdded` - Fired when a tool has been added to a chat
- `CodeCompanionToolStarted` - Fired when a tool has started executing
- `CodeCompanionToolFinished` - Fired when a tool has finished executing
- `CodeCompanionInlineStarted` - Fired at the start of the Inline strategy
- `CodeCompanionInlineFinished` - Fired at the end of the Inline strategy
- `CodeCompanionRequestStarted` - Fired at the start of any API request
- `CodeCompanionRequestStreaming` - Fired at the start of a streaming API request
- `CodeCompanionRequestFinished` - Fired at the end of any API request
- `CodeCompanionDiffAttached` - Fired when in Diff mode
- `CodeCompanionDiffDetached` - Fired when exiting Diff mode
- `CodeCompanionDiffAccepted` - Fired when a user accepts a change
- `CodeCompanionDiffRejected` - Fired when a user rejects a change

There are also events that can be utilized to trigger commands from within the
plugin:

- `CodeCompanionChatRefreshCache` - Used to refresh conditional elements in the chat buffer


EVENT DATA ~

Each event also comes with a data payload. For example, with
`CodeCompanionRequestStarted`:

>lua
    {
      buf = 10,
      data = {
        adapter = {
          formatted_name = "Copilot",
          model = "o3-mini-2025-01-31",
          name = "copilot"
        },
        bufnr = 10,
        id = 6107753,
        strategy = "chat"
      },
      event = "User",
      file = "CodeCompanionRequestStarted",
      group = 14,
      id = 30,
      match = "CodeCompanionRequestStarted"
    }
<

And the `CodeCompanionRequestFinished` also has a `data.status` value.


CONSUMING AN EVENT ~

Events can be hooked into as follows:

>lua
    local group = vim.api.nvim_create_augroup("CodeCompanionHooks", {})
    
    vim.api.nvim_create_autocmd({ "User" }, {
      pattern = "CodeCompanionInline*",
      group = group,
      callback = function(request)
        if request.match == "CodeCompanionInlineFinished" then
          -- Format the buffer after the inline request has completed
          require("conform").format({ bufnr = request.buf })
        end
      end,
    })
<

You can trigger an event with:

>lua
    vim.api.nvim_exec_autocmds("User", {
      pattern = "CodeCompanionChatRefreshCache",
    })
<


INLINE ASSISTANT                        *codecompanion-usage-inline-assistant*

As per the |codecompanion-getting-started-inline-assistant| guide, the inline
assistant enables you to code directly into a Neovim buffer. Simply run
`:CodeCompanion <your prompt>`, or make a visual selection to send that as
context to the LLM alongside your prompt.

For convenience, you can call prompts from the
|codecompanion-configuration-prompt-library| via the assistant. For example,
`:'<,'>CodeCompanion /tests` would ask the LLM to create some unit tests from
the selected text.


VARIABLES ~


  [!TIP] To ensure the LLM has enough context to complete a complex ask, it‚Äôs
  recommended to use the `buffer` variable
The inline assistant allows you to send context alongside your prompt via the
notion of variables:

- `buffer` - shares the contents of the current buffer
- `chat` - shares the LLM‚Äôs messages from the last chat buffer

Simply include them in your prompt. For example `:CodeCompanion #{buffer} add a
new method to this file`. Multiple variables can be sent as part of the same
prompt. You can even add your own custom variables as per the
|codecompanion-configuration-inline-assistant-variables|.


ADAPTERS ~

You can specify a different adapter to that in the configuration
(`strategies.inline.adapter`) when sending an inline prompt. Simply include the
adapter name as the first word in the prompt. For example `:<','>CodeCompanion
deepseek can you refactor this?`. This approach can also be combined with
variables.


CLASSIFICATION ~

One of the challenges with inline editing is determining how the LLM‚Äôs
response should be handled in the buffer. If you‚Äôve prompted the LLM to
`‚Äúcreate a table of 5 common text editors‚Äù` then you may wish for the
response to be placed at the cursor‚Äôs position in the current buffer.
However, if you asked the LLM to `‚Äúrefactor this function‚Äù` then you‚Äôd
expect the response to `replace` a visual selection. The plugin uses the inline
LLM you‚Äôve specified in your config to determine if the response should:

- `replace` - replace a visual selection you‚Äôve made
- `add` - be added in the current buffer at the cursor position
- `before` - to be added in the current buffer before the cursor position
- `new` - be placed in a new buffer
- `chat` - be placed in a chat buffer


DIFF MODE ~

By default, an inline assistant prompt will trigger the diff feature, showing
differences between the original buffer and the changes made by the LLM. This
can be turned off in your config via the `display.diff.provider` table. You can
also choose to accept or reject the LLM‚Äôs suggestions with the following
keymaps:

- `ga` - Accept an inline edit
- `gr` - Reject an inline edit

These keymaps can also be changed in your config via the
`strategies.inline.keymaps` table.


USER INTERFACE                            *codecompanion-usage-user-interface*

CodeCompanion aims to keep any changes to the user‚Äôs UI to a minimum.
Aesthetics, especially in Neovim, are highly subjective. So whilst it won‚Äôt
set much by default, it does endeavour to allow users to hook into the plugin
and customize the UI to their liking via |codecompanion-events|.

Below are some examples of how you can customize the UI related to
CodeCompanion.


PROGRESS UPDATES WITH FIDGET.NVIM BY @JESSEVDP ~

As per the discussion over at #813
<https://github.com/olimorris/codecompanion.nvim/discussions/813>.


INLINE SPINNER WITH FIDGET.NVIM BY @YUHUA99 ~

As per the comment on #640
<https://github.com/olimorris/codecompanion.nvim/discussions/640#discussioncomment-12866279>.


STATUS COLUMN EXTMARKS WITH THE INLINE ASSISTANT BY @LUCOBELLIC ~

As per the discussion over at #1297
<https://github.com/olimorris/codecompanion.nvim/discussions/1297>.


LUALINE.NVIM INTEGRATION ~

The plugin can be integrated with lualine.nvim to show an icon in the
statusline when a request is being sent to an LLM:

>lua
    local M = require("lualine.component"):extend()
    
    M.processing = false
    M.spinner_index = 1
    
    local spinner_symbols = {
      "‚†ã",
      "‚†ô",
      "‚†π",
      "‚†∏",
      "‚†º",
      "‚†¥",
      "‚†¶",
      "‚†ß",
      "‚†á",
      "‚†è",
    }
    local spinner_symbols_len = 10
    
    -- Initializer
    function M:init(options)
      M.super.init(self, options)
    
      local group = vim.api.nvim_create_augroup("CodeCompanionHooks", {})
    
      vim.api.nvim_create_autocmd({ "User" }, {
        pattern = "CodeCompanionRequest*",
        group = group,
        callback = function(request)
          if request.match == "CodeCompanionRequestStarted" then
            self.processing = true
          elseif request.match == "CodeCompanionRequestFinished" then
            self.processing = false
          end
        end,
      })
    end
    
    -- Function that runs every time statusline is updated
    function M:update_status()
      if self.processing then
        self.spinner_index = (self.spinner_index % spinner_symbols_len) + 1
        return spinner_symbols[self.spinner_index]
      else
        return nil
      end
    end
    
    return M
<


HEIRLINE.NVIM INTEGRATION ~

The plugin can also be integrated into heirline.nvim to show an icon when a
request is being sent to an LLM:

>lua
    local CodeCompanion = {
      static = {
        processing = false,
      },
      update = {
        "User",
        pattern = "CodeCompanionRequest*",
        callback = function(self, args)
          if args.match == "CodeCompanionRequestStarted" then
            self.processing = true
          elseif args.match == "CodeCompanionRequestFinished" then
            self.processing = false
          end
          vim.cmd("redrawstatus")
        end,
      },
      {
        condition = function(self)
          return self.processing
        end,
        provider = "Óç∞ ",
        hl = { fg = "yellow" },
      },
    }
<


WORKFLOWS                                      *codecompanion-usage-workflows*

Workflows can only be initiated from the |codecompanion-usage-action-palette|.
This is because they are a complex Lua table structure which needs to be
processed and added to a new chat buffer. Simply open up the Action Palette and
select your desired workflow.

You can create your own workflows by following the
|codecompanion-extending-workflows| guide.


==============================================================================
6. Extending                                         *codecompanion-extending*


CREATING ADAPTERS                  *codecompanion-extending-creating-adapters*


  [!TIP] Does your LLM state that it is "OpenAI Compatible"? If so, good news,
  you can extend from the `openai` adapter or use the `openai_compatible` one.
  Something we did with the xAI
  <https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/adapters/xai.lua>
  adapter
In CodeCompanion, adapters are interfaces that act as a bridge between the
plugin‚Äôs functionality and an LLM. All adapters must follow the interface,
below.

This guide is intended to serve as a reference for anyone who wishes to
contribute an adapter to the plugin or understand the inner workings of
existing adapters.

The plugin‚Äôs in-built adapters can be found here
<https://github.com/olimorris/codecompanion.nvim/tree/main/lua/codecompanion/adapters>.


THE INTERFACE ~

Let‚Äôs take a look at the interface of an adapter as per the `adapter.lua`
file:

>lua
    ---@class CodeCompanion.Adapter
    ---@field name string The name of the adapter e.g. "openai"
    ---@field formatted_name string The formatted name of the adapter e.g. "OpenAI"
    ---@field roles table The mapping of roles in the config to the LLM's defined roles
    ---@field url string The URL of the LLM to connect to
    ---@field env? table Environment variables which can be referenced in the parameters
    ---@field env_replaced? table Replacement of environment variables with their actual values
    ---@field headers table The headers to pass to the request
    ---@field parameters table The parameters to pass to the request
    ---@field body table Additional body parameters to pass to the request
    ---@field raw? table Any additional curl arguments to pass to the request
    ---@field opts? table Additional options for the adapter
    ---@field handlers table Functions which link the output from the request to CodeCompanion
    ---@field handlers.setup? fun()
    ---@field handlers.form_parameters fun()
    ---@field handlers.form_messages fun()
    ---@field handlers.chat_output fun()
    ---@field handlers.inline_output fun()
    ---@field handlers.on_exit? fun()
    ---@field handlers.teardown? fun()
    ---@field schema table Set of parameters for the LLM that the user can customise in the chat buffer
<

Everything up to the handlers should be self-explanatory. We‚Äôre simply
providing details of the LLM‚Äôs API to the curl library and executing the
request. The real intelligence of the adapter comes from the handlers table
which is a set of functions which bridge the functionality of the plugin to the
LLM.


ENVIRONMENT VARIABLES ~

When building an adapter, you‚Äôll need to inject variables into different
parts of the adapter class. If we take the Google Gemini
<https://github.com/google-gemini/cookbook/blob/main/quickstarts/rest/Streaming_REST.ipynb>
endpoint as an example, we need to inject the model and API key variables into
the URL of
`https://generativelanguage.googleapis.com/v1beta/models/${model}:streamGenerateContent?alt=sse&key=${api_key}`.
Whereas with OpenAI
<https://platform.openai.com/docs/api-reference/authentication>, we need an
`Authorization` http header to contain our API key.

Let‚Äôs take a look at the `env` table from the Google Gemini adapter that
comes with the plugin:

>lua
    url = "https://generativelanguage.googleapis.com/v1beta/models/${model}:streamGenerateContent?alt=sse&key=${api_key}",
    env = {
      api_key = "GEMINI_API_KEY",
      model = "schema.model.default",
    },
<

The key `api_key` represents the name of the variable which can be injected in
the adapter, and the value can represent one of:

- A command to execute on the user‚Äôs system
- An environment variable from the user‚Äôs system
- A function to be executed at runtime
- A path to an item in the adapter‚Äôs schema table
- A plain text value


  [!NOTE] Environment variables can be injected into the `url`, `headers` and
  `parameters` fields of the adapter class at runtime
**Commands**

An environment variable can be obtained from running a command on a user‚Äôs
system. This can be accomplished by prefixing the value with `cmd:` such as:

>lua
    env = {
      api_key = "cmd:op read op://personal/Gemini_API/credential --no-newline",
    },
<

In this example, we‚Äôre running the `op read` command to get a credential from
1Password.

**Environment Variable**

An environment variable can also be obtained by using lua‚Äôs `os.getenv`
function. Simply enter the name of the variable as a string such as:

>lua
    env = {
      api_key = "GEMINI_API_KEY",
    },
<

**Functions**

An environment variable can also be resolved via the use of a function such as:

>lua
    env = {
      api_key = function()
        return os.getenv("GEMINI_API_KEY")
      end,
    },
<

**Schema Values**

An environment variable can also be resolved by entering the path to a value in
a table on the adapter class. For example:

>lua
    env = {
      model = "schema.model.default",
    },
<

In this example, we‚Äôre getting the value of a user‚Äôs chosen model from the
schema table on the adapter.


HANDLERS ~

Currently, the handlers table requires four functions to be implemented:

- `form_parameters` - A function which can be used to set the parameters of the request
- `form_messages` - `Most` LLMs have a `messages` array in the body of the request which contains the conversation. This function can be used to format and structure that array
- `chat_output` - A function to format the output of the request into a Lua table that plugin can parse for the chat buffer
- `inline_output` - A function to format the output of the request into a Lua table that plugin can parse, inline, to the current buffer

There are some optional handlers which you can make use of:

- `on_exit` - A function which receives the full payload from the API and is run once the request completes. Useful for
    handling errors
- `tokens` - A function to determine the amount of tokens consumed in the request(s)
- `setup` - The function which is called before anything else
- `teardown` - A function which is called last and after the request has completed

Let‚Äôs take a look at a real world example of how we‚Äôve implemented the
OpenAI adapter.


  [!TIP] All of the adapters in the plugin come with their own tests. These serve
  as a great reference to understand how they‚Äôre working with the output of the
  API

OPENAI¬ÄÔøΩS API OUTPUT

If we reference the OpenAI documentation
<https://platform.openai.com/docs/guides/text-generation/chat-completions-api>
we can see that they require the messages to be in an array which consists of
`role` and `content`:

>sh
    curl https://api.openai.com/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -d '{
        "model": "gpt-4-0125-preview",
        "messages": [
          {
            "role": "user",
            "content": "Explain Ruby in two words"
          }
        ]
      }'
<


CHAT BUFFER OUTPUT

The chat buffer, which is structured like:

>markdown
    ## Me
    
    Explain Ruby in two words
<

results in the following output:

>lua
    {
      {
        role = "user",
        content = "Explain Ruby in two words"
      }
    }
<


FORM_MESSAGES

The chat buffer‚Äôs output is passed to this handler in for the form of the
`messages` parameter. So we can just output this as part of a messages table:

>lua
    handlers = {
      form_messages = function(self, messages)
        return { messages = messages }
      end,
    }
<


CHAT_OUTPUT

Now let‚Äôs look at how we format the output from OpenAI. Running that request
results in:

>txt
    data: {"id":"chatcmpl-90DdmqMKOKpqFemxX0OhTVdH042gu","object":"chat.completion.chunk","created":1709839462,"model":"gpt-4-0125-preview","system_fingerprint":"fp_70b2088885","choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}
<

>txt
    data: {"id":"chatcmpl-90DdmqMKOKpqFemxX0OhTVdH042gu","object":"chat.completion.chunk","created":1709839462,"model":"gpt-4-0125-preview","system_fingerprint":"fp_70b2088885","choices":[{"index":0,"delta":{"content":"Programming"},"logprobs":null,"finish_reason":null}]}
<

>txt
    data: {"id":"chatcmpl-90DdmqMKOKpqFemxX0OhTVdH042gu","object":"chat.completion.chunk","created":1709839462,"model":"gpt-4-0125-preview","system_fingerprint":"fp_70b2088885","choices":[{"index":0,"delta":{"content":" language"},"logprobs":null,"finish_reason":null}]},
<

>txt
    data: [DONE]
<


  [!IMPORTANT] Note that the `chat_output` handler requires a table containing
  `status` and `output` to be returned.
Remember that we‚Äôre streaming from the API so the request comes through in
batches. Thankfully the `http.lua` file handles this and we just have to handle
formatting the output into the chat buffer.

The first thing to note with streaming endpoints is that they don‚Äôt return
valid JSON. In this case, the output is prefixed with `data:`. CodeCompanion
comes with some handy utility functions to work with this:

>lua
    -- Put this at the top of your adapter
    local utils = require("codecompanion.utils.adapters")
    
    handlers = {
      chat_output = function(self, data)
        data = utils.clean_streamed_data(data)
      end
    }
<


  [!IMPORTANT] The data passed to the `chat_output` handler is the response from
  OpenAI
We can then decode the JSON using native vim functions:

>lua
    handlers = {
      chat_output = function(self, data)
        data = utils.clean_streamed_data(data)
        local ok, json = pcall(vim.json.decode, data, { luanil = { object = true } })
      end
    }
<

We want to include any nil values so we pass in `luanil = { object = true }`.

Examining the output of the API, we see that the streamed data is stored in a
`choices[1].delta` table. That‚Äôs easy to pickup:

>lua
    handlers = {
      chat_output = function(self, data)
        ---
        local delta = json.choices[1].delta
      end
    }
<

and we can then access the new streamed data that we want to write into the
chat buffer, with:

>lua
    handlers = {
      chat_output = function(self, data)
        local output = {}
        ---
        local delta = json.choices[1].delta
    
        if delta.content then
          output.content = delta.content
          output.role = delta.role or nil
        end
      end
    }
<

And then we can return the output in the following format:

>lua
    handlers = {
      chat_output = function(self, data)
        --
        return {
          status = "success",
          output = output,
        }
      end
    }
<

Now if we put it all together, and put some checks in place to make sure that
we have data in our response:

>lua
    handlers = {
      chat_output = function(self, data)
        local output = {}
    
        if data and data ~= "" then
          data = utils.clean_streamed_data(data)
          local ok, json = pcall(vim.json.decode, data, { luanil = { object = true } })
    
          local delta = json.choices[1].delta
    
          if delta.content then
            output.content = delta.content
            output.role = delta.role or nil
    
            return {
              status = "success",
              output = output,
            }
          end
        end
      end
    },
<


FORM_PARAMETERS

For the purposes of the OpenAI adapter, no additional parameters need to be
created. So we just pass this through:

>lua
    handlers = {
      form_parameters = function(self, params, messages)
        return params
      end,
    }
<


INLINE_OUTPUT

From a design perspective, the inline strategy is very similar to the chat
strategy. With the `inline_output` handler we simply return the content we wish
to be streamed into the buffer.

In the case of OpenAI, once we‚Äôve checked the data we have back from the LLM
and parsed it as JSON, we simply need to:

>lua
    ---Output the data from the API ready for inlining into the current buffer
    ---@param self CodeCompanion.Adapter
    ---@param data table The streamed JSON data from the API, also formatted by the format_data handler
    ---@param context table Useful context about the buffer to inline to
    ---@return string|table|nil
    inline_output = function(self, data, context)
      -- Data cleansed, parsed and validated
      -- ..
      local content = json.choices[1].delta.content
      if content then
        return content
      end
    end,
<

The `inline_output` handler also receives context from the buffer that
initiated the request.


ON_EXIT

Handling errors from a streaming endpoint can be challenging. It‚Äôs
recommended that any errors are managed in the `on_exit` handler which is
initiated when the response has completed. In the case of OpenAI, if there is
an error, we‚Äôll see a response back from the API like:

>sh
    data: {
    data:     "error": {
    data:         "message": "Incorrect API key provided: 1sk-F18b****************************************XdwS. You can find your API key at https://platform.openai.com/account/api-keys.",
    data:         "type": "invalid_request_error",
    data:         "param": null,
    data:         "code": "invalid_api_key"
    data:     }
    data: }
<

This would be challenging to parse! Thankfully we can leverage the `on_exit`
handler which receives the final payload, resembling:

>lua
    {
      body = '{\n    "error": {\n        "message": "Incorrect API key provided: 1sk-F18b****************************************XdwS. You can find your API key at https://platform.openai.com/account/api-keys.",\n        "type": "invalid_request_error",\n        "param": null,\n        "code": "invalid_api_key"\n    }\n}',
      exit = 0,
      headers = { "date: Thu, 03 Oct 2024 08:05:32 GMT" },
      status = 401
    }
<

and that‚Äôs much easier to work with:

>lua
    ---Function to run when the request has completed. Useful to catch errors
    ---@param self CodeCompanion.Adapter
    ---@param data table
    ---@return nil
    on_exit = function(self, data)
      if data.status >= 400 then
        log:error("Error: %s", data.body)
      end
    end,
<

The `log:error` call ensures that any errors are logged to the logfile as well
as displayed to the user in Neovim. It‚Äôs also important to reference that the
`chat_output` and `inline_output` handlers need to be able to ignore any errors
from the API and let `on_exit` handle them.


SETUP AND TEARDOWN

There are two optional handlers that you can make use of: `setup` and
`teardown`.

The `setup` handler will execute before the request is sent to the LLM‚Äôs
endpoint and before the environment variables have been set. This is leveraged
in the Copilot adapter to obtain the token before it‚Äôs resolved as part of
the environment variables table. The `setup` handler **must** return a boolean
value so the `http.lua` file can determine whether to proceed with the request.

The `teardown` handler will execute once the request has completed and after
`on_exit`.


THE UTILITY FILE

A lot of LLM endpoints claim to be "OpenAI Compatible" yet have odd quirks
which prevent you from using the OpenAI Adapter. Common issues can be:

- System messages have to be the first message (`anthropic`, `deepseek`)
- System messages have to be one message (`anthropic`, `deepseek`)
- Messages must follow a `User -> LLM -> User -> LLM` turn based flow (`deepseek`)

To address this, an adapter utilities
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/utils/adapters.lua>
file has been created that you can leverage in building or extending your own
adapters. Finally, always refer to the pre-built adapters as a reference point.


SCHEMA ~

The schema table describes the settings/parameters for the LLM. If the user has
`display.chat.show_settings = true` then this table will be exposed at the top
of the chat buffer.

We‚Äôll explore some of the options in the Copilot adapter‚Äôs schema table:

>lua
    schema = {
      model = {
        order = 1,
        mapping = "parameters",
        type = "enum",
        desc = "ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.",
        ---@type string|fun(): string
        default = "gpt-4o-2024-08-06",
        choices = {
          ["o3-mini-2025-01-31"] = { opts = { can_reason = true } },
          ["o1-2024-12-17"] = { opts = { can_reason = true } },
          ["o1-mini-2024-09-12"] = { opts = { can_reason = true } },
          "claude-3.5-sonnet",
          "claude-3.7-sonnet",
          "claude-3.7-sonnet-thought",
          "gpt-4o-2024-08-06",
          "gemini-2.0-flash-001",
        },
      },
    }
<

The model key sets out the specific model which is to be used to interact with
the Copilot endpoint. We‚Äôve listed the default, in this example, as
`gpt-4o-2024-08-06` but we allow the user to choose from a possible five
options, via the `choices` key. We‚Äôve given this an order value of `1` so
that it‚Äôs always displayed at the top of the chat buffer. We‚Äôve also given
it a useful description as this is used in the virtual text when a user hovers
over it. Finally, we‚Äôve specified that it has a mapping property of
`parameters`. This tells the adapter that we wish to map this model key to the
parameters part of the HTTP request. You‚Äôll also notice that some of the
models have a table attached to them. This can be useful if you need to do
conditional logic in any of the handler methods at runtime.

Let‚Äôs take a look at one more schema value:

>lua
    temperature = {
      order = 2,
      mapping = "parameters",
      type = "number",
      default = 0,
      ---@param self CodeCompanion.Adapter
      condition = function(self)
        local model = self.schema.model.default
        if type(model) == "function" then
          model = model()
        end
        return not vim.startswith(model, "o1")
      end,
      -- This isn't in the Copilot adapter but it's useful to reference!
      validate = function(n)
        return n >= 0 and n <= 2, "Must be between 0 and 2"
      end,
      desc = "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.",
    },
<

You‚Äôll see we‚Äôve specified a function call for the `condition` key. We‚Äôre
simply checking that the model name doesn‚Äôt start with `o1` as these models
don‚Äôt accept temperature as a parameter. You‚Äôll also see we‚Äôve specified
a function call for the `validate` key. We‚Äôre simply checking that the value
of the temperature is between 0 and 2.


CREATING PROMPTS                    *codecompanion-extending-creating-prompts*

The purpose of this guide is to showcase how you can extend the functionality
of CodeCompanion by adding your own prompts to the config that are reflected in
the `Action Palette`. The `Action Palette` is a lua table which is parsed by
the plugin and displayed as a `vim.ui.select` component. By specifying certain
keys, the behaviour of the table can be customised further.


ADDING A PROMPT TO THE PALETTE ~

A prompt can be added via the `setup` function:

>lua
    require("codecompanion").setup({
      prompt_library = {
        ["My New Prompt"] = {
          strategy = "chat",
          description = "Some cool custom prompt you can do",
          prompts = {
            {
              role = "system",
              content = "You are an experienced developer with Lua and Neovim",
            },
            {
              role = "user",
              content = "Can you explain why ..."
            }
          },
        }
      }
    })
<

In this example, if you run `:CodeCompanionActions`, you should see "My New
Prompt" in the bottom of the `Prompts` section of the palette. Clicking on your
new action will initiate the `chat` strategy and set the value of the chat
buffer based on the `role` and `content` that‚Äôs been specified in the prompt.

In the following sections, we‚Äôll explore how you can customise your prompts
even more.


RECIPE #1: CREATING BOILERPLATE CODE ~


BOILERPLATE HTML

As the years go by, I find myself writing less and less HTML. So when it comes
to quickly scaffolding out a HTML page, I inevitably turn to a search engine.
It would be great if I could have an action that could quickly generate some
boilerplate HTML from the `Action Palette`.

Let‚Äôs take a look at how we can achieve that:

>lua
    require("codecompanion").setup({
      prompt_library = {
        ["Boilerplate HTML"] = {
          strategy = "inline",
          description = "Generate some boilerplate HTML",
          opts = {
            mapping = "<LocalLeader>ch"
          },
          prompts = {
            {
              role = "system",
              content = "You are an expert HTML programmer",
            },
            {
              role = "user",
              content = "<user_prompt>Please generate some HTML boilerplate for me. Return the code only and no markdown codeblocks</user_prompt>",
            },
          },
        },
      },
    })
<

Nice! We‚Äôve used some careful prompting to ensure that we get HTML
boilerplate back from the LLM. Oh‚Ä¶and notice that I added a key map too! If
you plan on using the `inline` classification, it‚Äôs recommended to put your
prompt within `<user_prompt></user_prompt>` tags to make it explicit to the LLM
what your ask is.


LEVERAGING PRE-HOOKS

To make this example complete, we can leverage a pre-hook to create a new
buffer and set the filetype to be html:

>lua
    {
      ["Boilerplate HTML"] = {
        strategy = "inline",
        description = "Generate some boilerplate HTML",
        opts = {
          ---@return number
          pre_hook = function()
            local bufnr = vim.api.nvim_create_buf(true, false)
            vim.api.nvim_set_current_buf(bufnr)
            vim.api.nvim_set_option_value("filetype", "html", { buf = bufnr } )
            return bufnr
          end
        }
        ---
      }
    }
<

For the inline strategy, the plugin will detect a number being returned from
the `pre_hook` and assume that is the buffer number you wish any code to be
streamed into.


CONCLUSION

Whilst this example was useful at demonstrating the functionality of the
`Action Palette` and your custom prompts, it‚Äôs not using LLMs to add any real
value to your workflow (this boilerplate could be a snippet after all!). So
let‚Äôs step things up in the next section.


RECIPE #2: USING CONTEXT IN YOUR PROMPTS ~

Now let‚Äôs look at how we can use an LLM to advise us on some code that we
have visually selected in a buffer. Infact, this very example used to be
builtin to the plugin as the `Code Advisor` action:

>lua
    require("codecompanion").setup({
      prompt_library = {
        ["Code Expert"] = {
          strategy = "chat",
          description = "Get some special advice from an LLM",
          opts = {
            mapping = "<LocalLeader>ce",
            modes = { "v" },
            short_name = "expert",
            auto_submit = true,
            stop_context_insertion = true,
            user_prompt = true,
          },
          prompts = {
            {
              role = "system",
              content = function(context)
                return "I want you to act as a senior "
                  .. context.filetype
                  .. " developer. I will ask you specific questions and I want you to return concise explanations and codeblock examples."
              end,
            },
            {
              role = "user",
              content = function(context)
                local text = require("codecompanion.helpers.actions").get_code(context.start_line, context.end_line)
    
                return "I have the following code:\n\n```" .. context.filetype .. "\n" .. text .. "\n```\n\n"
              end,
              opts = {
                contains_code = true,
              }
            },
          },
        },
      },
    })
<

At first glance there‚Äôs a lot of new stuff in this. Let‚Äôs break it down.


PALETTE OPTIONS

>lua
    opts = {
      mapping = "<LocalLeader>ce",
      modes = { "v" },
      short_name = "expert",
      auto_submit = true,
      stop_context_insertion = true,
      user_prompt = true,
    },
<

In the `opts` table we‚Äôre specifying that we only want this action to appear
in the `Action Palette` if we‚Äôre in visual mode. We‚Äôre also asking the chat
strategy to automatically submit the prompts to the LLM via the `auto_submit =
true` value. We‚Äôre also telling the picker that we want to get the user‚Äôs
input before we action the response with `user_prompt = true`. With the
`short_name = "expert"` option, the user can run `:CodeCompanion /expert` from
the cmdline in order to trigger this prompt. Finally, as we define a prompt to
add any visually selected text to the chat buffer, we need to add the
`stop_context_insertion = true` option to prevent the chat buffer from
duplicating this. Remember that visually selecting text and opening a chat
buffer will result in that selection from being adding as a codeblock.


PROMPT OPTIONS AND CONTEXT

In the example below you can see how we‚Äôve structured the prompts to get
advice on the code:

>lua
    prompts = {
      {
        role = "system",
        content = function(context)
          return "I want you to act as a senior "
            .. context.filetype
            .. " developer. I will ask you specific questions and I want you to return concise explanations and codeblock examples."
        end,
      },
      {
        role = "user",
        content = function(context)
          local text = require("codecompanion.helpers.actions").get_code(context.start_line, context.end_line)
    
          return "I have the following code:\n\n```" .. context.filetype .. "\n" .. text .. "\n```\n\n"
        end,
        opts = {
          contains_code = true,
        }
      },
    },
<

One of the most useful features of the custom prompts is the ability to receive
context about the current buffer and any lines of code we‚Äôve selected. An
example context table looks like:

>lua
    {
      bufnr = 7,
      buftype = "",
      cursor_pos = { 10, 3 },
      end_col = 3,
      end_line = 10,
      filetype = "lua",
      is_normal = false,
      is_visual = true,
      lines = { "local function fire_autocmd(status)", '  vim.api.nvim_exec_autocmds("User", { pattern = "CodeCompanionInline", data = { status = status } })', "end" },
      mode = "V",
      start_col = 1,
      start_line = 8,
      winnr = 1000
    }
<

Using the context above, our first prompt then makes more sense:

>lua
    {
      role = "system",
      content = function(context)
        return "I want you to act as a senior "
          .. context.filetype
          .. " developer. I will ask you specific questions and I want you to return concise explanations and codeblock examples."
      end,
    },
<

We are telling the LLM to act as a "senior `Lua` developer" based on the
filetype of the buffer we initiated the action from.

Lets now take a look at the second prompt:

>lua
    {
      role = "user",
      content = function(context)
        local text = require("codecompanion.helpers.actions").get_code(context.start_line, context.end_line)
    
        return "I have the following code:\n\n```" .. context.filetype .. "\n" .. text .. "\n```\n\n"
      end,
      opts = {
        contains_code = true,
      }
    },
<

You can see that we‚Äôre using a handy helper to get the code between two lines
and formatting it into a markdown code block.


  [!IMPORTANT] We‚Äôve also specified a `contains_code = true` flag. If you‚Äôve
  turned off the sending of code to LLMs then the plugin will block this from
  happening.

CONDITIONALS

It‚Äôs also possible to conditionally set prompts via a `condition` function
that returns a boolean:

>lua
    {
      role = "user",
      ---
      condition = function(context)
        return context.is_visual
      end,
      ---
    },
<

And to determine the visibility of actions in the palette itself:

>lua
    {
      name = "Open chats ...",
      strategy = " ",
      description = "Your currently open chats",
      condition = function()
        return #require("codecompanion").buf_get_chat() > 0
      end,
      picker = {
        ---
      }
    }
<


OTHER OPTIONS ~


ALLOWING A PROMPT TO APPEAR AS A SLASH COMMAND

It can be useful to have a prompt from the prompt library appear as a slash
command in the chat buffer, like with the `Generate a Commit Message` action.
This can be done by specifying a `is_slash_cmd = true` option to the prompt:

>lua
    ["Generate a Commit Message"] = {
      strategy = "chat",
      description = "Generate a commit message",
      opts = {
        index = 9,
        is_default = true,
        is_slash_cmd = true,
        short_name = "commit",
        auto_submit = true,
      },
      prompts = {
        -- Prompts go here
      }
    }
<

In the chat buffer, if you type `/` you will see the value of `opts.short_name`
appear in the completion menu for you to expand.


SPECIFYING AN ADAPTER AND MODEL

>lua
    ["Your_New_Prompt"] = {
      strategy = "chat",
      description = "Your Special Prompt",
      opts = {
        adapter = {
          name = "ollama",
          model = "deepseek-coder:6.7b",
        },
      },
      -- Your prompts here
    }
<


SPECIFYING A PLACEMENT FOR INLINE PROMPTS

As outlined in the |codecompanion-usage-inline-assistant.html-classification|
section, an inline prompt can place its response in many different ways within
a Neovim buffer. To override this, you can reference a specific placement:

>lua
    ["Your_New_Prompt"] = {
      strategy = "inline",
      description = "Your Special Inline Prompt",
      opts = {
        placement = "new" -- or "replace"|"add"|"before"|"chat"
      },
      -- Your prompts here
    }
<

In this example, the LLM‚Äôs response will be placed in a new buffer.


IGNORING THE DEFAULT SYSTEM PROMPT

It may also be useful to create custom prompts that do not send the default
system prompt with the request:

>lua
    ["Your_New_Prompt"] = {
      strategy = "chat",
      description = "Your Special New Prompt",
      opts = {
        ignore_system_prompt = true,
      },
      -- Your prompts here
    }
<


PROMPTS WITH REFERENCES

It can be useful to pre-load a chat buffer with references to `files`,
`symbols` or even `urls`. This makes conversing with an LLM that much more
productive. As per `v11.9.0`, this can now be accomplished, as per the example
below:

>lua
    ["Test References"] = {
      strategy = "chat",
      description = "Add some references",
      opts = {
        index = 11,
        is_default = true,
        is_slash_cmd = false,
        short_name = "ref",
        auto_submit = false,
      },
      -- These will appear at the top of the chat buffer
      references = {
        {
          type = "file",
          path = { -- This can be a string or a table of values
            "lua/codecompanion/health.lua",
            "lua/codecompanion/http.lua",
          },
        },
        {
          type = "file",
          path = "lua/codecompanion/schema.lua",
        },
        {
          type = "symbols",
          path = "lua/codecompanion/strategies/chat/init.lua",
        },
        {
          type = "url", -- This URL will even be cached for you!
          url = "https://raw.githubusercontent.com/olimorris/codecompanion.nvim/refs/heads/main/lua/codecompanion/commands.lua",
        },
      },
      prompts = {
        {
          role = "user",
          content = "I'll think of something clever to put here...",
          opts = {
            contains_code = true,
          },
        },
      },
    },
<


CONCLUSION ~

Hopefully this serves as a useful introduction on how you can expand
CodeCompanion to create prompts that suit your workflow. It‚Äôs worth checking
out config.lua
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua>
files for more complex examples.


CREATING TOOLS                        *codecompanion-extending-creating-tools*

In CodeCompanion, tools offer pre-defined ways for LLMs to call functions on
your machine, acting as an Agent in the process. This guide walks you through
the implementation of tools, enabling you to create your own.

In the plugin, tools are a Lua table, consisting of various handler and output
functions, alongside a system prompt and an OpenAI compatible schema
<https://platform.openai.com/docs/guides/function-calling?api-mode=chat>.

When you add a tool to the chat buffer, this gives the LLM the knowledge to be
able to call the tool, when required. Once called, the plugin will parse the
LLM‚Äôs response and execute the tool accordingly, before sharing the output in
the chat buffer.


ARCHITECTURE ~

In order to create tools, you do not need to understand the underlying
architecture. However, for those who are curious about the implementation,
please see the diagram below:

>mermaid
    sequenceDiagram
        participant C as Chat Buffer
        participant L as LLM
        participant A as Agent
        participant E as Tool Executor
        participant T as Tool
    
        C->>L: Prompt
        L->>C: Response with Tool(s) request
    
        C->>A: Parse response
    
        loop For each detected tool
            A<<->>T: Resolve Tool config
            A->>A: Add Tool to queue
        end
    
        A->>E: Begin executing Tools
    
        loop While queue not empty
            E<<->>T: Fetch Tool implementation
    
            E->>E: Setup handlers and output functions
            T<<->>E: handlers.setup()
    
            alt
            Note over C,E: Some Tools require human approvals
                E->>C: Prompt for approval
                C->>E: User decision
            end
    
    
            alt
            Note over E,T: If Tool runs with success
                T-->>A: Update stdout
                E<<->>T: output.success()
            else
            Note over E,T: If Tool runs with errors
                T-->>A: Update stderr
                E<<->>T: output.error()
            end
    
            Note over E,T: When Tool completes
            E<<->>T: handlers.on_exit()
        end
    
        E-->>A: Fire autocmd
    
        A->>A: reset()
<


BUILDING YOUR FIRST TOOL ~

Before we begin, it‚Äôs important to familiarise yourself with the directory
structure of the agents and tools implementation:

>
    strategies/chat/agents
    ‚îú‚îÄ‚îÄ init.lua
    ‚îú‚îÄ‚îÄ executor/
    ‚îÇ   ‚îú‚îÄ‚îÄ cmd.lua
    ‚îÇ   ‚îú‚îÄ‚îÄ func.lua
    ‚îÇ   ‚îú‚îÄ‚îÄ init.lua
    ‚îÇ   ‚îú‚îÄ‚îÄ queue.lua
    ‚îú‚îÄ‚îÄ tools/
    ‚îÇ   ‚îú‚îÄ‚îÄ cmd_runner.lua
    ‚îÇ   ‚îú‚îÄ‚îÄ editor.lua
    ‚îÇ   ‚îú‚îÄ‚îÄ files.lua
<

When a tool is detected, the chat buffer sends any output to the
`agents/init.lua` file (I will commonly refer to that as the `‚Äúagent file‚Äù`
throughout this document). The agent file then parses the response from the
LLM, identifying the tool and duly executing it.

There are two types of tools that CodeCompanion can leverage:

1. **Command-based**: These tools can execute a series of commands in the background using a plenary.job <https://github.com/nvim-lua/plenary.nvim/blob/master/lua/plenary/job.lua>. They‚Äôre non-blocking, meaning you can carry out other activities in Neovim whilst they run. Useful for heavy/time-consuming tasks.
2. **Function-based**: These tools, like @insert_edit_into_file <https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/strategies/chat/tools/insert_edit_into_file.lua>, execute Lua functions directly in Neovim within the main process, one after another.

For the purposes of this section of the guide, we‚Äôll be building a simple
function-based calculator tool that an LLM can use to do basic maths.


TOOL STRUCTURE

All tools must implement the following structure which the bulk of this guide
will focus on explaining:

>lua
    ---@class CodeCompanion.Agent.Tool
    ---@field name string The name of the tool
    ---@field args table The arguments sent over by the LLM when making the function call
    ---@field cmds table The commands to execute
    ---@field function_call table The function call from the LLM
    ---@field schema table The schema that the LLM must use in its response to execute a tool
    ---@field system_prompt string | fun(schema: table): string The system prompt to the LLM explaining the tool and the schema
    ---@field opts? table The options for the tool
    ---@field env? fun(schema: table): table|nil Any environment variables that can be used in the *_cmd fields. Receives the parsed schema from the LLM
    ---@field handlers table Functions which handle the execution of a tool
    ---@field handlers.setup? fun(self: CodeCompanion.Agent.Tool, agent: CodeCompanion.Agent): any Function used to setup the tool. Called before any commands
    ---@field handlers.on_exit? fun(self: CodeCompanion.Agent.Tool, agent: CodeCompanion.Agent): any Function to call at the end of a group of commands or functions
    ---@field output? table Functions which handle the output after every execution of a tool
    ---@field output.prompt fun(self: CodeCompanion.Agent.Tool, agent: CodeCompanion.Agent): string The message which is shared with the user when asking for their approval
    ---@field output.rejected? fun(self: CodeCompanion.Agent.Tool, agent: CodeCompanion.Agent, cmd: table): any Function to call if the user rejects running a command
    ---@field output.error? fun(self: CodeCompanion.Agent.Tool, agent: CodeCompanion.Agent, cmd: table, stderr: table, stdout?: table): any The function to call if an error occurs
    ---@field output.success? fun(self: CodeCompanion.Agent.Tool, agent: CodeCompanion.Agent, cmd: table, stdout: table): any Function to call if the tool is successful
<


CMDS

**Command-Based Tools**

The `cmds` table is a collection of commands which the agent will execute one
after another, asynchronously, using plenary.job
<https://github.com/nvim-lua/plenary.nvim/blob/master/lua/plenary/job.lua>.

>lua
    cmds = {
      { "make", "test" },
      { "echo", "hello" },
    }
<

In this example, the plugin will execute `make test` followed by `echo hello`.
After each command executes, the plugin will automatically send the output to a
corresponding table on the agent file. If the command ran with success the
output will be written to `stdout`, otherwise it will go to `stderr`. We‚Äôll
be covering how you access that data in the output section below.

It‚Äôs also possible to pass in environment variables (from the `env` function)
by use of ${} brackets. The now removed `@code_runner` tool used them as below:

>lua
    cmds = {
        { "docker", "pull", "${lang}" },
        {
          "docker",
          "run",
          "--rm",
          "-v",
          "${temp_dir}:${temp_dir}",
          "${lang}",
          "${lang}",
          "${temp_input}",
        },
      },
    },
    ---@param self CodeCompanion.Agent.Tool
    ---@return table
    env = function(self)
      local temp_input = vim.fn.tempname()
      local temp_dir = temp_input:match("(.*/)")
      local lang = self.args.lang
      local code = self.args.code
    
      return {
        code = code,
        lang = lang,
        temp_dir = temp_dir,
        temp_input = temp_input,
      }
    end,
<


  [!IMPORTANT] Using the `handlers.setup()` function, it‚Äôs also possible to
  create commands dynamically like in the
  |codecompanion-usage-chat-buffer-agents.html-cmd-runner| tool.
**Function-based Tools**

Function-based tools use the `cmds` table to define functions that will be
executed one after another. Each function has four parameters, itself, the
actions request by the LLM, any input from a previous function call and a
`output_handler` callback for async execution. The `output_handler` handles the
result for an asynchronous tool. For a synchronous tool (like the calculator)
you can ignore it. For the purpose of our calculator example:

>lua
    cmds = {
      ---@param self CodeCompanion.Tool.Calculator The Calculator tool
      ---@param args table The arguments from the LLM's tool call
      ---@param input? any The output from the previous function call
      ---@return nil|{ status: "success"|"error", data: string }
      function(self, args, input)
        -- Get the numbers and operation requested by the LLM
        local num1 = tonumber(args.num1)
        local num2 = tonumber(args.num2)
        local operation = args.operation
    
        -- Validate input
        if not num1 then
          return { status = "error", data = "First number is missing or invalid" }
        end
    
        if not num2 then
          return { status = "error", data = "Second number is missing or invalid" }
        end
    
        if not operation then
          return { status = "error", data = "Operation is missing" }
        end
    
        -- Perform the calculation
        local result
        if operation == "add" then
          result = num1 + num2
        elseif operation == "subtract" then
          result = num1 - num2
        elseif operation == "multiply" then
          result = num1 * num2
        elseif operation == "divide" then
          if num2 == 0 then
            return { status = "error", data = "Cannot divide by zero" }
          end
          result = num1 / num2
        else
          return { status = "error", data = "Invalid operation: must be add, subtract, multiply, or divide" }
        end
    
        return { status = "success", data = result }
      end,
    },
<

For a synchronous tool, you only need to `return` the result table as
demonstrated. However, if you need to invoke some asynchronous actions in the
tool, you can use the `output_handler` to submit any results to the executor,
which will then invoke `output` functions to handle the results:

>lua
    cmds = {
      function(self, args, input, output_handler)
        -- This is for demonstration only
        vim.lsp.client.request(lsp_method, lsp_param, function(err, result, _, _)
          self.agent.chat:add_message({ role = "user", content = vim.json.encode(result) })
          output_handler({ status = "success", data = result })
        end, buf_nr)
      end
    }
<

Note that:

1. The `output_handler` will be called only once. Subsequent calls will be discarded;
2. A tool function should EITHER return the result table (synchronous), OR call the `output_handler` with the result table as the only argument (asynchronous), but not both.
If a function tries to both return the result and call the `output_handler`, the result will be undefined because there‚Äôs no guarantee which output will be handled first.

Similarly with command-based tools, the output is written to the `stdout` or
`stderr` tables on the agent file. However, with function-based tools, the user
must manually specify the outcome of the execution which in turn redirects the
output to the correct table:

>lua
    return { status = "error", data = "Invalid operation: must be add, subtract, multiply, or divide" }
<

Will cause execution of the tool to stop and populate `stderr` on the agent
file.

>lua
    return { status = "success", data = result }
<

Will populate the `stdout` table on the agent file and allow for execution to
continue.


SCHEMA

The function call that the LLM has sent, is parsed and sent to the `args`
parameter of any function you‚Äôve created in
|codecompanion-extending-tools.html-cmds|, as a JSON object which is then
converted to Lua via `vim.json.decode`. If the LLM has done its job correctly,
the Lua table should be the representation of what you‚Äôve described in the
schema. In summary, the schema represents the structure of the response that
the LLM must follow in order to call the tool.

For a tool to function correctly, your tool requires an OpenAI compatible
<https://platform.openai.com/docs/guides/function-calling?api-mode=chat>
schema. For our basic calculator tool, which does an operation on two numbers,
the schema could look something like:

>lua
    schema = {
      type = "function",
      ["function"] = {
        name = "calculator",
        description = "Perform simple mathematical operations on a user's machine",
        parameters = {
          type = "object",
          properties = {
            num1 = {
              type = "integer",
              description = "The first number in the calculation",
            },
            num2 = {
              type = "integer",
              description = "The second number in the calculation",
            },
            operation = {
              type = "string",
              enum = { "add", "subtract", "multiply", "divide" },
              description = "The mathematical operation to perform on the two numbers",
            },
          },
          required = {
            "num1",
            "num2",
            "operation"
          },
          additionalProperties = false,
        },
        strict = true,
      },
    },
<


SYSTEM_PROMPT

In the plugin, LLMs are given knowledge about a tool and how it can be used via
a system prompt. This method also informs the LLM on how to use the tool to
achieve a desired outcome.

For our calculator tool, our `system_prompt` could look something like:

>lua
    system_prompt = [[## Calculator Tool (`calculator`)
    
    ## CONTEXT
    - You have access to a calculator tool running within CodeCompanion, in Neovim.
    - You can use it to add, subtract, multiply or divide two numbers.
    
    ### OBJECTIVE
    - Do a mathematical operation on two numbers when the user asks
    
    ### RESPONSE
    - Always use the structure above for consistency.
    ]],
<


HANDLERS

The `handlers` table contains two functions that are executed before and after
a tool completes:

1. `setup` - Is called **before** anything in the |codecompanion-extending-tools.html-cmds| and |codecompanion-extending-tools.html-output| table. This is useful if you wish to set the cmds dynamically on the tool itself, like in the @cmd_runner <https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/strategies/chat/agents/tools/cmd_runner.lua> tool.
2. `on_exit` - Is called **after** everything in the |codecompanion-extending-tools.html-cmds| and |codecompanion-extending-tools.html-output| table.
3. `prompt_condition` - Is called **before** anything in the |codecompanion-extending-tools.html-cmds| and |codecompanion-extending-tools.html-output| table and is used to determine `if` the user should be prompted for approval. This is used in the `@insert_edit_into_file` tool to allow users to determine if they‚Äôd like to apply an approval to `buffer` or `file` edits.

For the purposes of our calculator, let‚Äôs just return some notifications so
you can see the agent and tool flow:

>lua
    handlers = {
      ---@param self CodeCompanion.Tool.Calculator
      ---@param agent CodeCompanion.Agent The tool object
      setup = function(self, agent)
        return vim.notify("setup function called", vim.log.levels.INFO)
      end,
      ---@param self CodeCompanion.Tool.Calculator
      ---@param agent CodeCompanion.Agent
      on_exit = function(self, agent)
        return vim.notify("on_exit function called", vim.log.levels.INFO)
      end,
    },
<


  [!TIP] The chat buffer can be accessed via `agent.chat` in the handler and
  output tables

OUTPUT

The `output` table enables you to manage and format output from the execution
of the |codecompanion-extending-tools.html-cmds|. It contains four functions:

1. `success` - Is called after `every` successful execution of a command/function. This can be a useful way of notifying the LLM of the success.
2. `error` - Is called when an error occurs whilst executing a command/function. It will only ever be called once as the whole execution of the |codecompanion-extending-tools.html-cmds| is halted. This can be a useful way of notifying the LLM of the failure.
3. `prompt` - Is called when user approval to execute the |codecompanion-extending-tools.html-cmds| is required. It forms the message prompt which the user is asked to confirm or reject.
4. `rejected` - Is called when a user rejects the approval to run the |codecompanion-extending-tools.html-cmds|. This method is used to inform the LLM of the rejection.

Let‚Äôs consider how me might implement this for our calculator tool:

>lua
    output = {
      ---@param self CodeCompanion.Tool.Calculator
      ---@param agent CodeCompanion.Agent
      ---@param cmd table The command that was executed
      ---@param stdout table
      success = function(self, agent, cmd, stdout)
        local chat = agent.chat
        return chat:add_tool_output(self, tostring(stdout[1]))
      end,
      ---@param self CodeCompanion.Tool.Calculator
      ---@param agent CodeCompanion.Agent
      ---@param cmd table
      ---@param stderr table The error output from the command
      ---@param stdout? table The output from the command
      error = function(self, agent, cmd, stderr, stdout)
        return vim.notify("An error occurred", vim.log.levels.ERROR)
      end,
    },
<

The `add_tool_output` method is designed to make it as easy as possible for
tool authors to update the message history on the chat buffer:

>lua
    ---Add the output from a tool to the message history and a message to the UI
    ---@param tool table The Tool that was executed
    ---@param for_llm string The output to share with the LLM
    ---@param for_user? string The output to share with the user. If empty will use the LLM's output
    ---@return nil
    function Chat:add_tool_output(tool, for_llm, for_user)
      -- Omitted for brevity
    end
<

The `for_llm` parameter is the string message that will be shared with the LLM
as part of the message history in the chat buffer, this is not made visible to
the user. The `for_user` parameter allows tool authors to customize the visible
output in the chat buffer, but if this is nil then the `for_llm` string is
used.


RUNNING THE CALCULATOR TOOL

If we put this all together in our config:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          tools = {
            calculator = {
              description = "Perform calculations",
              callback = {
                name = "calculator",
                cmds = {
                  ---@param self CodeCompanion.Tool.Calculator The Calculator tool
                  ---@param args table The arguments from the LLM's tool call
                  ---@param input? any The output from the previous function call
                  ---@return nil|{ status: "success"|"error", data: string }
                  function(self, args, input)
                    -- Get the numbers and operation requested by the LLM
                    local num1 = tonumber(args.num1)
                    local num2 = tonumber(args.num2)
                    local operation = args.operation
    
                    -- Validate input
                    if not num1 then
                      return { status = "error", data = "First number is missing or invalid" }
                    end
    
                    if not num2 then
                      return { status = "error", data = "Second number is missing or invalid" }
                    end
    
                    if not operation then
                      return { status = "error", data = "Operation is missing" }
                    end
    
                    -- Perform the calculation
                    local result
                    if operation == "add" then
                      result = num1 + num2
                    elseif operation == "subtract" then
                      result = num1 - num2
                    elseif operation == "multiply" then
                      result = num1 * num2
                    elseif operation == "divide" then
                      if num2 == 0 then
                        return { status = "error", data = "Cannot divide by zero" }
                      end
                      result = num1 / num2
                    else
                      return {
                        status = "error",
                        data = "Invalid operation: must be add, subtract, multiply, or divide",
                      }
                    end
    
                    return { status = "success", data = result }
                  end,
                },
                system_prompt = [[## Calculator Tool (`calculator`)
    
    ## CONTEXT
    - You have access to a calculator tool running within CodeCompanion, in Neovim.
    - You can use it to add, subtract, multiply or divide two numbers.
    
    ### OBJECTIVE
    - Do a mathematical operation on two numbers when the user asks
    
    ### RESPONSE
    - Always use the structure above for consistency.
    ]],
    
                schema = {
                  type = "function",
                  ["function"] = {
                    name = "calculator",
                    description = "Perform simple mathematical operations on a user's machine",
                    parameters = {
                      type = "object",
                      properties = {
                        num1 = {
                          type = "integer",
                          description = "The first number in the calculation",
                        },
                        num2 = {
                          type = "integer",
                          description = "The second number in the calculation",
                        },
                        operation = {
                          type = "string",
                          enum = { "add", "subtract", "multiply", "divide" },
                          description = "The mathematical operation to perform on the two numbers",
                        },
                      },
                      required = {
                        "num1",
                        "num2",
                        "operation",
                      },
                      additionalProperties = false,
                    },
                    strict = true,
                  },
                },
                handlers = {
                  ---@param self CodeCompanion.Tool.Calculator
                  ---@param agent CodeCompanion.Agent The tool object
                  setup = function(self, agent)
                    return vim.notify("setup function called", vim.log.levels.INFO)
                  end,
                  ---@param self CodeCompanion.Tool.Calculator
                  ---@param agent CodeCompanion.Agent
                  on_exit = function(self, agent)
                    return vim.notify("on_exit function called", vim.log.levels.INFO)
                  end,
                },
                output = {
                  ---@param self CodeCompanion.Tool.Calculator
                  ---@param agent CodeCompanion.Agent
                  ---@param cmd table The command that was executed
                  ---@param stdout table
                  success = function(self, agent, cmd, stdout)
                    local chat = agent.chat
                    return chat:add_tool_output(self, tostring(stdout[1]))
                  end,
                  ---@param self CodeCompanion.Tool.Calculator
                  ---@param agent CodeCompanion.Agent
                  ---@param cmd table
                  ---@param stderr table The error output from the command
                  ---@param stdout? table The output from the command
                  error = function(self, agent, cmd, stderr, stdout)
                    return vim.notify("An error occurred", vim.log.levels.ERROR)
                  end,
                },
              },
            },
          },
        }
      }
    })
<

and with the prompt:

>
    Use the @{calculator} tool for 100*50
<

You should see: `5000`, in the chat buffer.


ADDING IN USER APPROVALS

A big concern for users when they create and deploy their own tools is `‚Äúwhat
if an LLM does something I‚Äôm not aware of or I don‚Äôt approve?‚Äù`. To that
end, CodeCompanion tries to make it easy for a user to be the "human in the
loop" and approve tool use before execution.

To enable this for any tool, simply add the `requires_approval = true` in a
tool‚Äôs `opts` table:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          tools = {
            calculator = {
              description = "Perform calculations",
              callback = "as above",
              opts = {
                requires_approval = true,
              },
            }
          }
        }
      }
    })
<


  [!NOTE] `opts.requires_approval` can also be a function that receives the tool
  and agent classes as parameters
To account for the user being prompted for an approval, we can add a
`output.prompt` to the tool:

>lua
    output = {
      -- success and error functions remain the same ...
    
      ---The message which is shared with the user when asking for their approval
      ---@param self CodeCompanion.Tool.Calculator
      ---@param agent CodeCompanion.Agent
      ---@return string
      prompt = function(self, agent)
        return string.format(
          "Perform the calculation `%s`?",
          self.args.num1 .. " " .. self.args.operation .. " " .. self.args.num2
        )
      end,
    },
<

This will notify the user with the message: `Perform the calculation 100
multiply 50?`. The user can choose to proceed, reject or cancel. The latter
will cancel any tools from running.

You can also customize the output if a user rejects the approval:

>lua
    output = {
      -- success, error and prompt functions remain the same ...
    
      ---Rejection message back to the LLM
      ---@param self CodeCompanion.Tool.Calculator
      ---@param agent CodeCompanion.Agent
      ---@param cmd table
      ---@return nil
      rejected = function(self, agent, cmd)
        agent.chat:add_tool_output(self, "The user declined to run the calculator tool")
      end,
    },
<


OTHER TIPS ~


USE_HANDLERS_ONCE

If an LLM calls multiple tools in the same response, it‚Äôs possible that the
same tool may be called in succession. If you‚Äôd like to ensure that the
handler functions (`setup` and `on_exit`) are only called once, you can set
this in the `opts` table in the tool itself:

>lua
    return {
      name = "editor",
      opts = {
        use_handlers_once = true,
      },
      -- More code follows...
    }
<


CREATING WORKFLOWS                *codecompanion-extending-creating-workflows*

Workflows in CodeCompanion, are successive prompts which can be automatically
sent to the LLM in a turn-based manner. This allows for actions such as
reflection and planning to be easily implemented into your workflow. They can
be combined with tools to create agentic workflows, which could be used to
automate common activities like editing files and then running a test suite.

I fully recommend reading Issue 242 of The Batch
<https://www.deeplearning.ai/the-batch/issue-242/> to understand the origin of
workflows. They were originally implemented
<https://github.com/olimorris/codecompanion.nvim/commit/73e5a27075749b3ff60cfc796438d302d4b08715>
in the plugin as an early form of Chain-of-thought
<https://en.wikipedia.org/wiki/Prompt_engineering#Chain-of-thought> prompting,
via the use of reflection and planning prompts.


HOW THEY WORK ~

Before showcasing some examples, it‚Äôs important to understand how workflows
have been implemented in the plugin.

When initiated from the |codecompanion-usage-action-palette|, workflows attach
themselves to a |codecompanion-usage-chat-buffer-index| via the notion of a
`subscription`. That is, the workflow has subscribed to the conversation and
dataflow that‚Äôs taking place in the chat buffer. After the LLM sends a
response, the chat buffer will trigger an event on the subscription class. This
will execute a callback which has been defined in the workflow itself (often
times this is simply a text prompt), and the event will duly be deleted from
the subscription to prevent it from being executed again.


SIMPLE WORKFLOWS ~

Workflows are setup in exactly the same way as prompts in the
|codecompanion-extending-prompts|. Take the `code workflow` as an example:

>lua
    ["Code workflow"] = {
      strategy = "workflow",
      description = "Use a workflow to guide an LLM in writing code",
      opts = {
        index = 4,
        is_default = true,
        short_name = "cw",
      },
      prompts = {
        {
          -- We can group prompts together to make a workflow
          -- This is the first prompt in the workflow
          -- Everything in this group is added to the chat buffer in one batch
          {
            role = constants.SYSTEM_ROLE,
            content = function(context)
              return string.format(
                "You carefully provide accurate, factual, thoughtful, nuanced answers, and are brilliant at reasoning. If you think there might not be a correct answer, you say so. Always spend a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. Don't be verbose in your answers, but do provide details and examples where it might help the explanation. You are an expert software engineer for the %s language",
                context.filetype
              )
            end,
            opts = {
              visible = false,
            },
          },
          {
            role = constants.USER_ROLE,
            content = "I want you to ",
            opts = {
              auto_submit = false,
            },
          },
        },
        -- This is the second group of prompts
        {
          {
            role = constants.USER_ROLE,
            content = "Great. Now let's consider your code. I'd like you to check it carefully for correctness, style, and efficiency, and give constructive criticism for how to improve it.",
            opts = {
              auto_submit = false,
            },
          },
        },
        -- This is the final group of prompts
        {
          {
            role = constants.USER_ROLE,
            content = "Thanks. Now let's revise the code based on the feedback, without additional explanations.",
            opts = {
              auto_submit = false,
            },
          },
        },
      },
    },
<

You‚Äôll notice that the comments use the notion of "groups". These are
collections of prompts which are added to a chat buffer in a timely manner.
Infact, the second group will only be added once the LLM has responded to the
first group‚Ä¶and so on.

The `auto_submit` option allows you to automatically send prompts to an LLM,
saving you a keypress. Please note that there is default delay of 2s (which can
be changed as per the config.lua
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua>
file under `opts.submit_delay`) to avoid triggering a rate limit block on an
LLM‚Äôs endpoint.


AGENTIC WORKFLOWS ~

By combining a workflow with tools, we can use an LLM to act as an Agent and do
some impressive things!

A great example of that is the `Edit<->Test` workflow that come with the
plugin. This workflow asks the LLM to edit code in a buffer and then run a test
suite, feeding the output back to the LLM to then make future edits if
required.

Let‚Äôs breakdown the prompts in that workflow:

>lua
    prompts = {
      {
        {
          name = "Setup Test",
          role = constants.USER_ROLE,
          opts = { auto_submit = false },
          content = function()
            -- Leverage auto_tool_mode which disables the requirement of approvals and automatically saves any edited buffer
            vim.g.codecompanion_auto_tool_mode = true
    
            -- Some clear instructions for the LLM to follow
            return [[### Instructions
    
    Your instructions here
    
    ### Steps to Follow
    
    You are required to write code following the instructions provided above and test the correctness by running the designated test suite. Follow these steps exactly:
    
    1. Update the code in #{buffer}{watch} using the @{insert_edit_into_file} tool
    2. Then use the @{cmd_runner} tool to run the test suite with `<test_cmd>` (do this after you have updated the code)
    3. Make sure you trigger both tools in the same response
    
    We'll repeat this cycle until the tests pass. Ensure no deviations from these steps.]]
          end,
        },
      },
      --- Prompts to be continued ...
    },
<

The first prompt in a workflow should set the ask of the LLM and provide clear
instructions. In this case, we‚Äôre giving the LLM access to the
|codecompanion-usage-chat-buffer-agents.html-files| and
|codecompanion-usage-chat-buffer-agents.html-cmd-runner| tools to edit a buffer
and run tests, respectively.

We‚Äôre giving the LLM knowledge of the buffer with the `#buffer` variable and
also telling CodeCompanion to watch it for any changes with the `{watch}`
parameter. Prior to sending a response to the LLM, the plugin will share any
changes to that buffer, keeping the LLM updated.

Now let‚Äôs look at how we trigger the automated reflection prompts:

>lua
    {
      {
        --- Prompts continued...
        {
          {
            name = "Repeat On Failure",
            role = constants.USER_ROLE,
            opts = { auto_submit = true },
            -- Scope this prompt to only run when the cmd_runner tool is active
            condition = function()
              return _G.codecompanion_current_tool == "cmd_runner"
            end,
            -- Repeat until the tests pass, as indicated by the testing flag
            repeat_until = function(chat)
              return chat.tools.flags.testing == true
            end,
            content = "The tests have failed. Can you edit the buffer and run the test suite again?",
          },
        },
      },
    },
<

Now there‚Äôs a little bit more to unpack in this prompt. Firstly, we‚Äôre
automatically submitting the prompt to the LLM to save the user some time and
keypresses. Next, we‚Äôre scoping the prompt to only be sent to the chat buffer
if the currently active tool is the
|codecompanion-usage-chat-buffer-agents.html-cmd-runner|.

We‚Äôre also leveraging a function called `repeat_until`. This ensures that the
prompt is always attached to the chat buffer until a condition is met. In this
case, until the tests pass. In the
|codecompanion-usage-chat-buffer-agents.html-cmd-runner| tool, we ask the LLM
to pass a flag if it detects a test suite is being run. The plugin picks up on
that flag and puts the test outcome into the chat buffer class as a flag.

Finally, we‚Äôre letting the LLM know that the tests failed, and asking it to
fix.


USEFUL OPTIONS ~

There are also a number of options which haven‚Äôt been covered in the example
prompts above:

**Specifying an Adapter**

You can specify a specific adapter for a workflow prompt:

>lua
    ["Workflow"] = {
      strategy = "workflow",
      description = "My workflow",
      opts = {
        adapter = "openai", -- Always use the OpenAI adapter for this workflow
      },
      -- Prompts go here
    },
<

**Persistent Prompts**

By default, all workflow prompts are of the type `once`. That is, they are
consumed once and then removed. However, this can be changed:

>lua
    ["A Cool Workflow"] = {
      strategy = "workflow",
      description = "My cool workflow",
      prompts = {
        {
          -- Some first prompt
        },
        {
          {
            role = constants.USER_ROLE,
            content = "This prompt will never go away!",
            type = "persistent",
            opts = {
              auto_submit = false,
            },
          },
        },
      },
    },
<

Note that persistent prompts are not available for the first prompt group.


CREATING WORKSPACES              *codecompanion-extending-creating-workspaces*

Workspaces act as a context management system for your project. This context
sits in a `codecompanion-workspace.json` file in the root of the current
working directory. For the purposes of this guide, the file will be referred to
as the `workspace file`.

For the origin of workspaces in CodeCompanion, and why I settled on this
design, please see the original
<https://github.com/olimorris/codecompanion.nvim/discussions/705> announcement.


STRUCTURE ~

The workspace file primarily consists of a groups array and data objects. A
group defines a specific feature or functionality within the code base, which
is made up of a number of individual data objects. These objects are simply a
reference to code, which could be in the form of a file, a symbolic outline, or
a URL.

The exact JSON schema for a workspace file can be seen here
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/workspace-schema.json>
and below is an extract of CodeCompanion‚Äôs own workspace file:

>json
    {
      "name": "CodeCompanion.nvim",
      "version": "1.0.0",
      "system_prompt": "CodeCompanion.nvim is an AI-powered productivity tool integrated into Neovim, designed to enhance the development workflow by seamlessly interacting with various large language models (LLMs). It offers features like inline code transformations, code creation, refactoring, and supports multiple LLMs such as OpenAI, Anthropic, and Google Gemini, among others. With tools for variable management, agents, and custom workflows, CodeCompanion.nvim streamlines coding tasks and facilitates intelligent code assistance directly within the Neovim editor.",
      "groups": [
        {
          "name": "Chat Buffer",
          "system_prompt": "I've grouped a number of files together into a group I'm calling \"${group_name}\". The chat buffer is a Neovim buffer which allows a user to interact with an LLM. The buffer is formatted as Markdown with a user's content residing under a H2 header. The user types their message, saves the buffer and the plugin then uses Tree-sitter to parse the buffer, extracting the contents and sending to an adapter which connects to the user's chosen LLM. The response back from the LLM is streamed into the buffer under another H2 header. The user is then free to respond back to the LLM.\n\nBelow are the relevant files which we will be discussing:\n\n${group_files}",
          "opts": {
            "remove_config_system_prompt": true
          },
          "data": ["chat-buffer-init", "chat-references", "chat-watchers"]
        },
      ],
      "data": {
        "chat-buffer-init": {
          "type": "file",
          "path": "lua/codecompanion/strategies/chat/init.lua",
          "description": "The `${filename}` file is the entry point for the chat strategy. All methods directly relating to the chat buffer reside here."
        },
        "chat-references": {
          "type": "symbols",
          "path": "lua/codecompanion/strategies/chat/references.lua",
          "description": "References are files, buffers, symbols or URLs that are shared with an LLM to provide additional context. The `${filename}` is where this logic sits and I've shared its symbolic outline below."
        },
        "chat-watchers": {
          "type": "symbols",
          "path": "lua/codecompanion/strategies/chat/watchers.lua",
          "description": "A watcher is when a user has toggled a specific buffer to be watched. When a message is sent to the LLM by the user, any changes made to the watched buffer are also sent, giving the LLM up to date context. The `${filename}` is where this logic sits and I've shared its symbolic outline below."
        },
      }
    }
<

- The `system_prompt` value contains the prompt that will be sent to the LLM as a system prompt
- The `groups` array contains the grouping of data that will be shared with the LLM.
- The `data` object contains the files that will be shared as part of the group

When a user leverages the workspace slash command in the chat buffer, the
high-level system prompt is added as a message, followed by the system prompt
from the group. After that, the individual items in the data array on the group
are added along with their descriptions as a regular user prompt.


GROUPS ~

Groups are the core of the workspace file. They are where logical groupings of
data are defined. Exploring the `Chat Buffer` group in detail:

>json
    {
      "name": "Chat Buffer",
      "system_prompt": "I've grouped a number of files together into a group I'm calling \"${group_name}\". The chat buffer is a Neovim buffer which allows a user to interact with an LLM. The buffer is formatted as Markdown with a user's content residing under a H2 header. The user types their message, saves the buffer and the plugin then uses Tree-sitter to parse the buffer, extracting the contents and sending to an adapter which connects to the user's chosen LLM. The response back from the LLM is streamed into the buffer under another H2 header. The user is then free to respond back to the LLM.\n\nBelow are the relevant files which we will be discussing:\n\n${group_files}",
      "opts": {
        "remove_config_system_prompt": true
      },
      "data": ["chat-buffer-init", "chat-references", "chat-watchers"]
    }
<

There‚Äôs a lot going on in there:

- Firstly, the `system_prompt` within the group is a way of adding to the main, workspace system prompt
- The `remove_config_system_prompt` is a way of telling the plugin to exclude its own, default system prompt

Let‚Äôs also explore one of the `data` objects in detail:

>json
    {
      "data": {
        "chat-buffer-init": {
          "type": "file",
          "path": "lua/codecompanion/strategies/chat/init.lua",
          "description": "The `${filename}` file is the entry point for the chat strategy. All methods directly relating to the chat buffer reside here."
        }
      }
    }
<

- We‚Äôre specifying a |codecompanion-extending-workspace.html-data-types| of `file` which is explained in more detail below. The type can be one of `file`, `symbols` or `url`
- We‚Äôre outlining the `path` to the file within the current working directory
- We‚Äôre providing description which gets sent alongside the contents of the file as part of a user prompt. We‚Äôre also leveraging a `${filename}` variable which is explained in more detail in the |codecompanion-extending-workspace.html-variables| section below


DATA TYPES ~


FILES

When `files` are defined, their entire content is shared with the LLM alongside
the description. This is useful for files where a deep understanding of how
they function is required. Of course, this can consume significant tokens.
CodeCompanion will automatically detect if a file is open in Neovim, as a
buffer, and load it as such. This makes it more convenient to leverage watchers
and pins and keep an LLM updated when you modify the contents.


SYMBOLS

The plugin uses Tree-sitter queries
<https://github.com/olimorris/codecompanion.nvim/tree/main/queries> to create a
symbolic outline of files, capturing:

- Classes, methods, and interfaces
- Function names
- File/library imports
- Start/end lines for each symbol

Alongside the `@files` tool group, the LLM can request specific line ranges
from these symbols - a cost-effective alternative to sharing entire files.


URLS

Workspace files also support the loading of data from URLs. When loading a URL,
the `/fetch` slash command adapter retrieves the data. The plugin:

- Caches URL data to disk by default
- Prompts before restoring from cache
- Can be configured with:
    - `"ignore_cache": true` to never use cache
    - `"auto_restore_cache": true` to always use cache without prompting

An example of using the configuration options:

>json
    {
      "minitest-docs": {
        "type": "url",
        "path": "https://raw.githubusercontent.com/echasnovski/mini.nvim/refs/heads/main/TESTING.md",
        "description": "Below is the Mini.Test documentation:",
        "opts": {
          "auto_restore_cache": true
        }
      }
    }
<


VARIABLES ~

A list of all the variables available in workspace files:

- `${workspace_name}` - The name of the workspace file
- `${group_name}` - The name of the group that is being processed by the slash command
- `${filename}` - The name of the current file/symbol that is being processed
- `${cwd}` - The current working directory
- `${path}` - The path to the current file/symbol/url

When building your workspace file, you can create a `vars` object which
contains custom variables for use elsewhere in file. For example:

>json
    {
      "name": "CodeCompanion.nvim",
      "version": "1.0.0",
      "system_prompt": "Workspace system prompt",
      "vars": {
        "test_desc": "This is a test description",
        "stubs": "tests/stubs"
      },
      "groups": [
        {
          "name": "Test",
          "description": "${test_desc}",
          "system_prompt": "Test group system prompt",
          "data": ["stub-go"]
        }
      ],
      "data": {
        "stub-go": {
          "type": "file",
          "path": "${stubs}/example.go",
          "description": "An example Go file"
        },
      }
    }
<


GENERATING A WORKSPACE FILE ~

The plugin comes with an
|codecompanion-usage-action-palette.html-default-prompts| prompt to help you
generate a workspace file. It will open up a chat buffer and add the workspace
JSON schema as part of a prompt. It will also determine if you have a workspace
file in your current working directory, and if you do, the prompts will be
altered to ask the LLM to help you in adding a group, rather than generating a
whole workspace file.

Whilst this approach is helpful, you‚Äôll still need to manually share a lot of
context for the LLM to be able to understand the intricacies of your codebase.
A more optimal way is to leverage VectorCode
<https://github.com/Davidyz/VectorCode>. The prompt will determine if you have
this installed and add it to the chat as a tool.

Remember, the key objective with a workspace file is to rapidly share context
with an LLM, making it‚Äôs response more accurate and more useful.


CREATING EXTENSIONS              *codecompanion-extending-creating-extensions*

CodeCompanion supports extensions similar to telescope.nvim, allowing users to
create functionality that can be shared with others. Extensions can either be
distributed as plugins or defined locally in your configuration.


EXTENSIONS ~

Extensions are configured in your CodeCompanion setup:

>lua
    -- Install the extension
    {
      "olimorris/codecompanion.nvim",
      dependencies = {
        "author/codecompanion_history.nvim" -- history extension
      }
    }
    
    -- Configure in your setup
    require("codecompanion").setup({
      extensions = {
        codecompanion_history = {
          enabled = true, -- defaults to true
          opts = {
            history_file = vim.fn.stdpath("data") .. "/codecompanion_chats.json",
            max_history = 10, -- maximum number of chats to keep
          }
        }
      }
    })
<


CREATING EXTENSIONS ~

Extensions are typically distributed as plugins. Create a new plugin with the
following structure:

>
    your-extension/
    ‚îú‚îÄ‚îÄ lua/
    ‚îÇ   ‚îî‚îÄ‚îÄ codecompanion/
    ‚îÇ       ‚îî‚îÄ‚îÄ _extensions/
    ‚îÇ           ‚îî‚îÄ‚îÄ your_extension/
    ‚îÇ               ‚îî‚îÄ‚îÄ init.lua  -- Main extension file
    ‚îî‚îÄ‚îÄ README.md
<

The init.lua file should export a module that provides setup and optional
exports:

>lua
    ---@class CodeCompanion.Extension
    ---@field setup fun(opts: table) Function called when extension is loaded
    ---@field exports? table Functions exposed via codecompanion.extensions.your_extension
    local Extension = {}
    
    ---Setup the extension
    ---@param opts table Configuration options 
    function Extension.setup(opts)
      -- Initialize extension
      -- Add actions, keymaps etc.
    end
    
    -- Optional: Functions exposed via codecompanion.extensions.your_extension
    Extension.exports = {
      clear_history = function() end
    }
    
    return Extension
<


EXTENDING CHAT FUNCTIONALITY

A common pattern is to add keymaps, slash_commands, tools to the
codecompanion.config object inside setup function.

>lua
    ---This is called on codecompanion setup.
    ---You can access config via require("codecompanion.config") and chat via require("codecompanion.chat").last_chat() etc
    function Extension.setup(opts)
      -- Add action to chat keymaps
      local chat_keymaps = require("codecompanion.config").strategies.chat.keymaps
      
      chat_keymaps.open_saved_chats = {
        modes = {
          n = opts.keymap or "gh", 
        },
        description = "Open Saved Chats",
        callback = function(chat)
            -- Implementation of opening saved chats
            vim.notify("Opening saved chats for " .. chat.id)
        end
      }
    end
<

Once configured, extension exports are accessible via:

>lua
    local codecompanion = require("codecompanion")
    -- Use exported functions
    codecompanion.extensions.codecompanion_history.clear_history()
<


LOCAL EXTENSIONS ~

Extensions can also be defined directly in your configuration for simpler use
cases:

>lua
    -- Example: Adding a message editor extension
    require("codecompanion").setup({
      extensions = {
        editor = {
          enabled = true,
          opts = {},
          callback = {
            setup = function(ext_config)
              -- Add a new action to chat keymaps
              local open_editor = {
                modes = {
                  n = "ge",  -- Keymap to open editor
                },
                description = "Open Editor",
                callback = function(chat)
                  -- Implementation of editor opening logic
                  -- You have access to the chat buffer via the chat parameter
                  vim.notify("Editor opened for chat " .. chat.id)
                end,
              }
    
              -- Add the action to chat keymaps config
              local chat_keymaps = require("codecompanion.config").strategies.chat.keymaps
              chat_keymaps.open_editor = open_editor
            end,
    
            -- Optional: Expose functions
            exports = {
              is_editor_open = function()
                return false -- Implementation
              end
            }
          }
        }
      }
    })
<

The callback can be: - A function returning the extension table - The extension
table directly - A string path to a module that returns the extension


DYNAMIC REGISTRATION ~

Extensions can also be added dynamically using

>lua
    require("codecompanion").register_extension("codecompanion_history", {
        callback = {
            setup = function()
            end,
            exports = {}
        },
    })
<


BEST PRACTICES ~

1. **Namespacing**:- Use unique names for extensions to avoid conflicts
- Prefix functions and variables appropriately


2. **Configuration**:- Provide sensible defaults
- Allow customization via opts table
- Document all options


3. **Integration**:- Follow CodeCompanion‚Äôs patterns for actions and tools
- Use existing utilities like keymaps.set_keymap
- Handle errors appropriately


4. **Documentation**:- Document installation process
- List all available options
- Provide usage examples



Generated by panvimdoc <https://github.com/kdheepak/panvimdoc>

vim:tw=78:ts=8:noet:ft=help:norl:
