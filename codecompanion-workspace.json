{
  "name": "CodeCompanion.nvim",
  "version": "1.0.0",
  "system_prompt": "CodeCompanion.nvim is an AI-powered productivity tool that seamlessly integrates large language models (LLMs) into the Neovim editing experience. The plugin is built around three core strategies: **Chat** (conversational interface), **Inline** (direct code generation), and **Tools** (tool execution).\n\n**Architecture Philosophy:**\n- **Adapter Pattern**: Unified interface for multiple LLM providers (OpenAI, Anthropic, Google Gemini, etc.)\n- **Strategy Pattern**: Different interaction modes (chat, inline, tools) with shared infrastructure\n- **Builder Pattern**: Complex UI operations broken into composable, testable components\n- **Event-Driven**: Extensible through subscribers, watchers, and workflows\n\n**Key Components:**\n- **Chat Buffer**: Markdown-formatted conversational interface with Tree-sitter parsing\n- **Adapters**: LLM-specific handlers for requests, responses, and capabilities\n- **Tools**: Function calling system for LLM-driven automation\n- **Variables & Context**: Context injection system for enhanced prompts\n- **Workflows**: Automated prompt chaining and response processing\n\n**Testing & Quality:**\nExtensively tested using Mini.Test with both unit tests and visual regression testing through screenshots. The codebase emphasizes maintainability through clear separation of concerns, comprehensive error handling, and modular design patterns that make extending functionality straightforward.",
  "groups": [
    {
      "name": "Chat Buffer",
      "system_prompt": "I've grouped a number of files together into a group I'm calling \"${group_name}\". The chat buffer is a Neovim buffer which allows a user to interact with an LLM. The buffer is formatted as Markdown with a user's content residing under a H2 header. The user types their message, saves the buffer and the plugin then uses Tree-sitter to parse the buffer, extracting the contents and sending to an adapter which connects to the user's chosen LLM. The response back from the LLM is streamed into the buffer under another H2 header. The user is then free to respond back to the LLM.\n\nBelow are the relevant files which we will be discussing:\n\n${group_files}",
      "opts": {
        "remove_config_system_prompt": true
      },
      "data": ["chat-buffer-init", "chat-messages"]
    },
    {
      "name": "Chat UI",
      "system_prompt": "The Chat UI group contains the files responsible for rendering and formatting content in the chat buffer. This includes the message builder pattern that coordinates how different types of content (tool output, reasoning, standard messages) are formatted and displayed to the user.\n\nKey components:\n1. **Builder Pattern**: The main orchestrator that handles the flow of adding headers, formatting content, and writing to the buffer with centralized state management\n2. **Formatters**: Specialized classes that handle different message types (tools, reasoning, standard content)\n3. **UI Management**: Methods for handling buffer operations, folding, and visual presentation\n4. **State Management**: Rich formatting state objects that track role changes, content transitions, and section boundaries\n5. **Section Detection**: Logic for identifying when new sections are needed (e.g., LLM message â†’ tool output transitions)\n\nThe files to analyze are:\n${group_files}",
      "opts": {
        "remove_config_system_prompt": true
      },
      "data": [
        "chat-ui-builder",
        "chat-ui-formatters-base",
        "chat-ui-formatters-tools",
        "chat-ui-formatters-reasoning",
        "chat-ui-formatters-standard",
        "chat-ui-folds",
        "chat-ui-icons",
        "chat-ui-builder-test"
      ]
    },
    {
      "name": "Workflows",
      "system_prompt": "Within the plugin, workflows are a way for users to be able to automatically send or chain multiple prompts, sequentially, to an LLM. They do this by \"subscribing\" to a chat buffer.\n\nFocus on:\n 1. How workflows integrate with the chat buffer\n2. How they can be refactored\n3. How they can work better with the chat buffer itself.\n\nThe files to analyze are:\n${group_files}",
      "opts": {
        "remove_config_system_prompt": true
      },
      "data": [
        "strategies-init",
        "chat-subscribers",
        "chat-buffer-init",
        "workflow-example"
      ]
    },
    {
      "name": "Tests",
      "system_prompt": "The plugin uses a testing framework called Mini.Test. The tests are written in Lua and are located in the `tests` directory. The tests are run using the `make test` command. The tests are written in a BDD style and are used to ensure the plugin is functioning as expected.",
      "opts": {
        "remove_config_system_prompt": true
      },
      "data": ["test-helpers", "minitest-docs", "test-screenshot-example"]
    },
    {
      "name": "Adapters",
      "system_prompt": "In the CodeCompanion plugin, adapters are used to connect to LLMs or Agents. HTTP adapters contain various options for the LLM's endpoint alongside a defined schema for properties such as the model, temperature, top k, top p etc. HTTP adapters also contain various handler functions which define how messages which are sent to the LLM should be formatted alongside how output from the LLM should be received and displayed in the chat buffer. The adapters are defined in the `adapters` directory.",
      "opts": {
        "remove_config_system_prompt": true
      },
      "data": [
        "adapters-init",
        "adapters-shared",
        "adapters-http",
        "http-client",
        "adapters-test"
      ]
    },
    {
      "name": "Inline",
      "system_prompt": "In the CodeCompanion plugin, the inline strategy allows user's to prompt LLMs to write code directly into a Neovim buffer. To make the experience as smooth as possible, the user can just send a prompt like 'refactor this class' and the LLM will generate code to answer the question, alongside providing a determination on where to place the code. This is called the placement.",
      "opts": {
        "remove_config_system_prompt": true
      },
      "data": [
        "inline-init",
        "http-client",
        "adapters-openai",
        "commands",
        "inline-test"
      ]
    },
    {
      "name": "Tools",
      "system_prompt": "In the CodeCompanion plugin, tools can be leveraged by an LLM to execute lua functions or shell commands on the users machine. CodeCompanion uses an LLM's native function calling to receive a response in JSON, parse the response and call the corresponding tool. This feature has been implemented via the tools/init.lua file, which passes all of the tools and adds them to a queue. Then those tools are run consecutively by the orchestrator.lua file.",
      "opts": {
        "remove_config_system_prompt": true
      },
      "data": [
        "tool-system-init",
        "orchestrator",
        "runtime-runner",
        "runtime-queue",
        "queue-example",
        "read_file_tool",
        "read_file_tool_test"
      ]
    },
    {
      "name": "ACP Integration",
      "system_prompt": "This group covers the ACP (Agent Communication Protocol) integration in CodeCompanion.nvim. ACP enables session-based, schema-driven communication with LLM agents, supporting authentication, streaming responses, tool calls, and permission handling. Key components:\n\n- **ACPAdapter**: Encapsulates ACP-specific logic for agent communication.\n- **ACPConnection**: Manages agent process, session lifecycle, authentication, and streaming.\n- **PromptBuilder**: Fluent API for sending prompts and handling streamed responses, tool calls, and errors.\n- **Chat Buffer**: Parses user messages, submits prompts via ACP, and streams agent responses into the buffer.\n\nRelevant files:\n- `lua/codecompanion/acp.lua` (ACPConnection, PromptBuilder)\n- `lua/codecompanion/adapters/acp/init.lua` (ACPAdapter)\n- `lua/codecompanion/strategies/chat/init.lua` (Chat Buffer logic)\n- [ACP JSON Schema](.codecompanion/acp_json_schema.json) (for schema awareness)\n\nRefer to the ACP documentation for message flow, schema types, and extensibility.",
      "opts": {
        "remove_config_system_prompt": true
      },
      "data": [
        "acp-implementation-notes",
        "acp-schema",
        "acp-connection",
        "acp-adapter",
        "acp-handler",
        "acp-permissions",
        "acp-fs",
        "gemini-cli"
      ]
    }
  ],
  "data": {
    "chat-buffer-init": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/init.lua",
      "description": "The `${filename}` file is the entry point for the chat strategy. All methods directly relating to the chat buffer reside here."
    },
    "chat-buffer-ui": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/ui/init.lua",
      "description": "The `${filename}` file is responsible for translating the content in the chat buffer into the Neovim UI that the user interacts with. It handles the rendering of any headers alongside holding methods that relate to folding and extmarks"
    },
    "chat-messages": {
      "type": "file",
      "path": "tests/stubs/messages.lua",
      "description": "Messages in the chat buffer are lua table objects as seen in `${filename}`. They contain the role of the message (user, assistant, system), the content of the message, and any additional metadata such as visibility and sometimes type. The latter are important when data is being written to the buffer with `add_buf_message` as they allow the chat ui builder pattern to determine the type of message that it's receiving and therefore determine how to format it. In the example shared in `${filename}`, you can see how a user has prompted the LLM to use a tool and the chain of messages that lead to the LLM's final response."
    },
    "chat-context": {
      "type": "symbols",
      "path": "lua/codecompanion/strategies/chat/context.lua",
      "description": "Context is files, buffers, symbols or URLs that are shared with an LLM to provide additional context. The `${filename}` is where this logic sits and I've shared its symbolic outline below."
    },
    "chat-watchers": {
      "type": "symbols",
      "path": "lua/codecompanion/strategies/chat/watchers.lua",
      "description": "A watcher is when a user has toggled a specific buffer to be watched. When a message is sent to the LLM by the user, any changes made to the watched buffer are also sent, giving the LLM up to date context. The `${filename}` is where this logic sits and I've shared its symbolic outline below."
    },
    "acp-handler": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/acp/handler.lua",
      "description": "Extracted ACP submission logic from the chat buffer. Handles ACP connection management, message streaming with multiple handlers (message chunks, thought chunks, tool calls), output collection, error handling, and completion logic. This separation keeps the main chat buffer focused on orchestration while isolating ACP-specific complexity."
    },
    "acp-permissions": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/acp/permissions.lua",
      "description": "Contains helper functions that can be called by the handler to manage permissions from the chat buffer with an ACP adapter and Neovim itself. For example, showing diffs, setting keymaps to accept/reject diffs etc."
    },
    "acp-fs": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/acp/fs.lua",
      "description": "File which handles the fs/* commands from the ACP schema. This includes the reading and writing of files in the current working directory of the user's machine"
    },
    "chat-ui-builder": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/ui/builder.lua",
      "description": "The message builder coordinates the entire process of adding content to the chat buffer. It uses a fluent interface to chain operations: adding headers when roles change, formatting content through specialized formatters, writing to the buffer with proper folding, and updating internal state. It is called from the chat buffer's `add_buf_message` method which occurs throughout the codebase."
    },
    "chat-ui-formatters-base": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/ui/formatters/base.lua",
      "description": "The base formatter class that defines the interface all formatters must implement. It requires formatters to implement `can_handle`, `get_tag`, and `format` methods. Each formatter receives the chat instance, allowing access to state like `last_tag` and `has_reasoning_output`."
    },
    "chat-ui-formatters-tools": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/ui/formatters/tools.lua",
      "description": "Handles formatting of tool output messages. It manages spacing rules (extra line breaks after LLM messages), calculates fold information for multi-line tool output, and ensures proper visual separation between tool results and other content types."
    },
    "chat-ui-formatters-reasoning": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/ui/formatters/reasoning.lua",
      "description": "Formats reasoning content from LLMs that support chain-of-thought responses. It adds the '### Reasoning' header only once per reasoning sequence and manages the `_has_reasoning_output` state to coordinate with the standard formatter for proper transitions."
    },
    "chat-ui-formatters-standard": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/ui/formatters/standard.lua",
      "description": "The fallback formatter that handles regular message content. It manages transitions from reasoning to response content (adding '### Response' headers), handles spacing after tool output, and processes standard text content with proper line splitting."
    },
    "chat-ui-folds": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/ui/folds.lua",
      "description": "Manages visual presentation of tool output and chat context including folding functionality. It creates collapsible folds for tool output with custom fold text that shows success/failure icons and summarized content alongside folding the chat context at the top of the chat buffer. The fold text adapts based on keywords in the tool output to indicate success or failure states."
    },
    "chat-ui-icons": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/ui/icons.lua",
      "description": "Manages the overlaying of icons in the chat buffer for tools, based on their status. It manages this by applying extmarks in a given position which places the icon at the start of a given line. The icons are then used to indicate the status of a tool, whether it was successful or not"
    },
    "chat-ui-builder-test": {
      "type": "file",
      "path": "tests/strategies/chat/test_builder.lua",
      "description": "Comprehensive tests for the builder pattern covering state management, section detection, reasoning transitions, and header logic. Tests verify that the builder correctly manages formatting state across multiple message additions and properly detects when new sections or headers are needed."
    },
    "strategies-init": {
      "type": "file",
      "path": "lua/codecompanion/strategies/init.lua",
      "description": "The `${filename}` is where the workflow are initiated from."
    },
    "chat-subscribers": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/subscribers.lua",
      "description": "The `${filename}` is where the subscribers logic resides."
    },
    "workflow-example": {
      "type": "file",
      "path": "tests/stubs/workflow.lua",
      "description": "An example workflow can be seen below:"
    },
    "test-helpers": {
      "type": "file",
      "path": "tests/helpers.lua",
      "description": "I've included the test helper file as well:"
    },
    "test-screenshot-example": {
      "type": "file",
      "path": "tests/adapters/http/test_tools_in_chat_buffer.lua",
      "description": "I've included an example test file that highlights the capability of mini.test and the approach I've taken to testing in CodeCompanion. Whilst I'm not a fan of using screenshots for all tests, in this case it was useful as it made up part of a much larger integration test."
    },
    "minitest-docs": {
      "type": "url",
      "path": "https://raw.githubusercontent.com/echasnovski/mini.nvim/refs/heads/main/TESTING.md",
      "description": "Below is the Mini.Test documentation:",
      "opts": {
        "auto_restore_cache": true
      }
    },
    "adapters-init": {
      "type": "file",
      "path": "lua/codecompanion/adapters/init.lua",
      "description": "Currently CodeCompanion supports http adapters."
    },
    "adapters-shared": {
      "type": "file",
      "path": "lua/codecompanion/adapters/shared.lua",
      "description": "There are some functions that are shared between the two adapter types and these are contained in this file."
    },
    "adapters-http": {
      "type": "file",
      "path": "lua/codecompanion/adapters/http/init.lua",
      "description": "This is the logic for the HTTP adapters. Various logic sits within this file which allows the adapter to be resolved into a CodeCompanion.HTTPAdapter object before it's used throughout the plugin to connect to an LLM endpoint."
    },
    "adapters-test": {
      "type": "file",
      "path": "tests/adapters/test_adapters.lua",
      "description": "This is a test file for adapter's broadly, which might give you more of an understand of what is required by an adapter deep within CodeCompanion."
    },
    "http-client": {
      "type": "file",
      "path": "lua/codecompanion/http.lua",
      "description": "HTTP Adapters are then passed to the http client which sends requests to LLMs via Curl:"
    },
    "schema": {
      "type": "file",
      "path": "lua/codecompanion/schema.lua",
      "description": "Adapters must follow a schema. The validation and how schema values are extracted from the table schema is defined in:"
    },
    "inline-init": {
      "type": "file",
      "path": "lua/codecompanion/strategies/inline/init.lua",
      "description": "This is the entry point for the inline strategy. All methods directly relating to the inline strategy reside here. I'm in the process of refactoring this which is where I will seek your guidance. You can see how it leverages adapters to connect to an LLM and receive the response before feeding it back into a Neovim buffer."
    },
    "adapters-openai": {
      "type": "file",
      "path": "lua/codecompanion/adapters/openai.lua",
      "description": "I'm including an example adapter. In this case for OpenAI. You'll see that it has a handler called 'inline_output' which handles how the output is fed back to the inline strategy."
    },
    "commands": {
      "type": "file",
      "path": "lua/codecompanion/commands.lua",
      "description": "There are many entry points to make an inline edit. Most commonly is via the `:CodeCompanion` command:"
    },
    "inline-test": {
      "type": "file",
      "path": "tests/strategies/inline/test_inline.lua",
      "description": "Including a link to the test file:"
    },
    "tool-system-init": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/tools/init.lua",
      "description": "This is the entry point for the tool system. If an LLM's response includes a function call (or tool call) then this file is triggered which in turns add tools to a queue before calling the orchestrator"
    },
    "orchestrator": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/tools/orchestrator.lua",
      "description": "The orchestrator file then runs the tools in the queue, whether they're functions or commands:"
    },
    "runtime-runner": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/tools/runtime/runner.lua",
      "description": "This is how function tools are run:"
    },
    "runtime-queue": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/tools/runtime/queue.lua",
      "description": "This is the queue implementation"
    },
    "queue-example": {
      "type": "file",
      "path": "tests/stubs/queue.txt",
      "description": "This is how the queue object can look. This is an example of a function tool, a command tool, followed by a function tool:"
    },
    "read_file_tool": {
      "type": "file",
      "path": "lua/codecompanion/strategies/chat/tools/catalog/read_file.lua",
      "description": "This is an example of a tool in CodeCompanion that reads a file in the current working directory. It's a great example of a function tool."
    },
    "read_file_tool_test": {
      "type": "file",
      "path": "tests/strategies/chat/tools/catalog/test_read_file.lua",
      "description": "This is the corresponding test for the read file tool."
    },
    "acp-connection": {
      "type": "file",
      "path": "lua/codecompanion/acp.lua",
      "description": "Implements ACPConnection and PromptBuilder for session management, prompt submission, and streaming agent responses."
    },
    "acp-adapter": {
      "type": "file",
      "path": "lua/codecompanion/adapters/acp/init.lua",
      "description": "Defines the ACPAdapter abstraction for ACP-specific agent communication."
    },
    "acp-schema": {
      "type": "file",
      "path": ".codecompanion/acp_json_schema.json",
      "description": "ACP JSON schema defining request, response, and notification types for agent communication."
    },
    "acp-implementation-notes": {
      "type": "file",
      "path": ".codecompanion/acp.md",
      "description": "This document explains how ACP works in CodeCompanion.nvim."
    },
    "gemini-cli": {
      "type": "file",
      "path": "lua/codecompanion/adapters/acp/gemini_cli.lua",
      "description": "An example ACP implementation for Google Gemini CLI, demonstrating how to use the ACPAdapter with a specific LLM."
    }
  }
}
